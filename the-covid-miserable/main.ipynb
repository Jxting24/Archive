{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2fdlo2cYrXr"
   },
   "source": [
    "# The Covid: Miserable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tr-4QgGtYrXw"
   },
   "source": [
    "### Installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3766,
     "status": "ok",
     "timestamp": 1640180774843,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "VFTjI-OWYrXy",
    "outputId": "abd22f17-e456-4db8-a3ac-3482c92a29e0"
   },
   "outputs": [],
   "source": [
    "%pip install matplotlib==3.5.1 lightgbm==3.3.1 tensorflow==2.7.0 numpy==1.20.0 xgboost==1.5.1 pandas==1.3.5 scikit-learn==1.0.1 mlxtend==0.19.0 keras_tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fEvDI3SYv7-"
   },
   "source": [
    "### Connect Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15700,
     "status": "ok",
     "timestamp": 1640180794634,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "kqKBBpp7Y2v0",
    "outputId": "8a7b42c5-ad36-462b-948e-28f26c24254d"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2xss6m1Y5kw"
   },
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kDf5IQ-Y6-g"
   },
   "outputs": [],
   "source": [
    "current_directory = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaF936sLYrXz"
   },
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLq83axgYrX0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "from scipy.integrate import solve_ivp\n",
    "from sklearn import *\n",
    "from sklearn import base\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from numba import jit\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import warnings; warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iddkTb48YrX1"
   },
   "source": [
    "### Data\n",
    "From authorize sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nap2LB7YrX1"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1640180866526,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "_UgdRvvrYrX2",
    "outputId": "1e8f3743-c380-4f88-9219-914102bd77c0"
   },
   "outputs": [],
   "source": [
    "def prepare_and_preprocessing():\n",
    "    data = pd.read_csv(current_directory + 'datasets/covid.csv')\n",
    "    data['date'] = pd.to_datetime(data['date'], dayfirst=True)\n",
    "    fdata = data.drop(columns='date').copy()\n",
    "    data = data[data['date'] > '2020-04-01'].set_index('date')\n",
    "    assert type(data.index) == pd.core.indexes.datetimes.DatetimeIndex\n",
    "    \n",
    "    return fdata, data\n",
    "\n",
    "fdata, data = prepare_and_preprocessing()\n",
    "display(data.loc['2021-07-08', :].to_frame().T, fdata.iloc[78, :].to_frame().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pWa-OL0YrX3"
   },
   "source": [
    "### Plotting Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1640180869528,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "DgQkafXLYrX3",
    "outputId": "c37371d9-8ede-4862-dad9-4c755206ef53"
   },
   "outputs": [],
   "source": [
    "from cycler import cycler\n",
    "custom_style = {\n",
    "    'figure.autolayout': True,      # Figure automatically adjust layout\n",
    "    'figure.titlesize': 20,         # Figure suptitle font size\n",
    "    'figure.figsize': (10, 5),      # Figure figsize\n",
    "    'figure.dpi': 100,              # Figure dots per inch\n",
    "    'axes.spines.top': False,       # Draw Axis spines top\n",
    "    'axes.spines.left': False,      # Draw Axis spin left  \n",
    "    'axes.titlesize': 10,           # Axes title font size\n",
    "    'axes.titlelocation': 'left',   # Axes title alignment\n",
    "    'axes.labelsize': 14,           # Axes label size\n",
    "    'axes.grid': True,              # Axes grid\n",
    "    'axes.prop_cycle': cycler(color=['#d73027', '#00518b', '#b1ef89', '#ffd500', '#000000']),\n",
    "    'grid.color': '#969696',        # Axes grid col\n",
    "    'xtick.direction': 'inout',     # Xtick direction\n",
    "    'ytick.direction': 'inout',     # Ytick direction\n",
    "    'xtick.minor.visible': True,    # Draw Xtick minor \n",
    "    'ytick.minor.visible': True,    # Draw Ytick minor \n",
    "    'ytick.right': True,            # Draw ticks right\n",
    "    'ytick.left': False,            # Draw ticks left\n",
    "    'ytick.labelright': True,       # Draw ticks label right\n",
    "    'ytick.labelleft': False,       # Draw ticks label left\n",
    "    'xaxis.labellocation': 'right', # Xaxis alignment\n",
    "    'yaxis.labellocation': 'top',   # Yaxis alignment\n",
    "    'font.family': 'monospace',     # Figure font \n",
    "    'legend.fontsize': 10,          # Legend font size\n",
    "    'legend.loc': 'best',           # Legend location\n",
    "}\n",
    "\n",
    "custom_style2 = {\n",
    "    'xtick.major.size': 7,\n",
    "    'xtick.minor.size': 3.5,\n",
    "    'xtick.major.width': 1.1,\n",
    "    'xtick.minor.width': 1.1,\n",
    "    'xtick.major.pad': 5,\n",
    "    'xtick.minor.visible': True,\n",
    "    'xtick.top': False,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.major.size': 7,\n",
    "    'ytick.minor.size': 3.5,\n",
    "    'ytick.major.width': 1.1,\n",
    "    'ytick.minor.width': 1.1,\n",
    "    'ytick.major.pad': 5,\n",
    "    'ytick.minor.visible': True,\n",
    "    'ytick.right': True,\n",
    "    'ytick.labelsize': 12,\n",
    "    'font.family': 'Palatino Linotype',\n",
    "    'font.size': 16,\n",
    "    'ytick.right': True,            \n",
    "    'ytick.left': False,            \n",
    "    'ytick.labelright': True,       \n",
    "    'ytick.labelleft': False, \n",
    "    'xaxis.labellocation': 'right', \n",
    "    'yaxis.labellocation': 'top',  \n",
    "    'axes.spines.top': False,     \n",
    "    'axes.spines.left': False,    \n",
    "}\n",
    "\n",
    "print('========================================')\n",
    "print('Auto Configured:')\n",
    "print('========================================')\n",
    "for k, v in zip(list(custom_style.keys()), list(custom_style.values())):\n",
    "    n = 30 - len(k)\n",
    "    print(str(k) + str(' '*n) + str(v))\n",
    "print('========================================')\n",
    "print('Further Configure: ')\n",
    "print('========================================')\n",
    "print('ax.yaxis.set_label_position(\"right\")')\n",
    "print('mpl.rcParams.update(mpl.rcParamsDefault)')\n",
    "  \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='svg'\n",
    "\n",
    "plt.style.use(custom_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNlBfy8aYrX5"
   },
   "source": [
    "### SEIR Model\n",
    "\n",
    "SEIR model, a compartment model for modelling infectious diseases spread where the total population is assigned to either Susceptible, Exposed, Infectious and Recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPZmw1m-YrX5"
   },
   "outputs": [],
   "source": [
    "skipped_window = 21\n",
    "total_infected = data['total_cases'][skipped_window:]\n",
    "total_deceased = data['total_deceased'][skipped_window:]\n",
    "total_recovered = data['total_recovered'][skipped_window:]\n",
    "\n",
    "@jit(nopython=True)\n",
    "def SEIR(t, y, beta, gamma, sigma, alpha, t_quarantine):\n",
    "    \n",
    "    N = np.int(33e6 / (10/0.55))\n",
    "    \n",
    "    S = y[0] # Susceptible\n",
    "    E = y[1] # Exposed\n",
    "    I = y[2] # Infected\n",
    "    R = y[3] # Recovered\n",
    "    \n",
    "    if(t > t_quarantine):\n",
    "        beta_t = beta*np.exp(-alpha*(t-t_quarantine))\n",
    "    else:\n",
    "        beta_t = beta\n",
    "        \n",
    "    dS = - beta_t * S * I / N\n",
    "    dE = beta_t * S * I / N - sigma * E\n",
    "    dI = sigma * E - gamma * I\n",
    "    dR = gamma * I\n",
    "    \n",
    "    return [dS, dE, dI, dR]\n",
    "\n",
    "def fitting_SEIR(vec, t_q, N, test_size):\n",
    "    beta, gamma, sigma, alpha = vec\n",
    "    \n",
    "    t_f = total_infected.shape[0]\n",
    "    y0 = [N-total_infected[0], 0, total_infected[0], 0]\n",
    "    t_eval = np.arange(0, t_f, 1)\n",
    "    N = np.int(33e6 / (10/0.55))\n",
    "    \n",
    "    sol = solve_ivp(SEIR, [0, t_f], y0, args=(beta, gamma, sigma, alpha, t_q), t_eval=t_eval)\n",
    "    \n",
    "    split = np.int((1-test_size) * total_infected.shape[0])\n",
    "    \n",
    "    error = (\n",
    "        np.sum(\n",
    "            5 * (total_deceased[:split] + total_recovered[:split] - sol.y[3][:split]) ** 2) +    \n",
    "        np.sum(\n",
    "            (total_infected[:split] - np.cumsum(sol.y[1][:split] + sol.y[2][:split])) ** 2)\n",
    "    ) / split\n",
    "    \n",
    "    return error\n",
    "\n",
    "def interpret_SEIR(plot=True):\n",
    "    N = np.int(33e6 / (10/0.55))\n",
    "    t_q = 21\n",
    "    t_f = total_infected.shape[0]\n",
    "    y0 = [N-total_infected[0], 0, total_infected[0], 0]\n",
    "    t_eval = np.arange(0, t_f, 1)\n",
    "    test_size = 0.1\n",
    "    opts = minimize(fitting_SEIR, [2, 1, 0.8, 0.3], method='Nelder-Mead', args=(t_q, N, test_size))\n",
    "    beta, gamma, sigma, alpha = opts.x\n",
    "    sol = solve_ivp(SEIR, [0, t_f], y0, args=(beta, gamma, sigma, alpha, t_q), t_eval=t_eval)\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "        ax.plot(data.index[skipped_window:], np.cumsum(sol.y[1]+sol.y[2]), label='Exposed and Infected')\n",
    "        ax.plot(data.index[skipped_window:], total_infected, label='Infected')\n",
    "        ax.plot(data.index[skipped_window:], sol.y[3], label='Recovered')\n",
    "        plt.suptitle('SEIR Model: An Assumption', ha='left', x=.015, y=.95, fontsize=20); \n",
    "        plt.legend(); plt.xlabel('Date'); plt.ylabel('Population'); ax.yaxis.set_label_position('right')\n",
    "        plt.title('Skipped Window: {} | Quarantine Take Place {} days after outbreak'.format(skipped_window, t_q))\n",
    "        plt.show()\n",
    "    \n",
    "interpret_SEIR(plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OE1YDYPaYrX6"
   },
   "source": [
    "### Exploratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3RI3T1-YrX6"
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def percent_change(df, target_columns):\n",
    "    assert type(df) == pd.core.frame.DataFrame\n",
    "    assert type(target_columns) == str\n",
    "    percent_change = df[target_columns].pct_change()*100\n",
    "    return percent_change\n",
    "\n",
    "def absolute_change(df, target_columns):\n",
    "    assert type(df) == pd.core.frame.DataFrame\n",
    "    assert type(target_columns) == str\n",
    "    absolute_change = df[target_columns] - df[target_columns].shift(1)\n",
    "    return absolute_change\n",
    "\n",
    "def style_negative(v, props=''):\n",
    "    return props if v < 0 else None\n",
    "\n",
    "def style_positive(v, props=''):\n",
    "    return props if v > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1640180956779,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "6DZiseWtYrX7",
    "outputId": "6d5d3958-b9ea-45bc-db1d-0fd0bcf2add4"
   },
   "outputs": [],
   "source": [
    "def changes_in_series(df):\n",
    "    data = df.copy()\n",
    "    print('Stats In a Glance')\n",
    "    needed = ['tests', 'cases', 'deceased', 'vaccination', 'recovered']\n",
    "    for c in needed:\n",
    "        data[c+' Percent Change'] = percent_change(data, c)\n",
    "    \n",
    "    percent_change_cols = [col for col in data.columns if 'Percent Change' in col]\n",
    "    display(data[percent_change_cols].tail(20).style.applymap(style_negative, props='color:red;')\\\n",
    "                                                    .applymap(style_positive, props='color:green;').format(na_rep='Missing'))\n",
    "    \n",
    "changes_in_series(df=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jJUL0VgYrX7"
   },
   "source": [
    "### Logistic Function\n",
    "\n",
    "Additional to SEIR general mathematic model, Logistic Function can be also used to estimate the inflection point and the overall trend of the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygNp4JUEYrX7"
   },
   "outputs": [],
   "source": [
    "def logistic_function(x, k, x_0, maxy):\n",
    "    return maxy / (1 + np.exp(-k*(x-x_0))) # k = growth rate | x_0 = inflection point | maxy = the curve maximum value\n",
    "\n",
    "def logistic_pred(df, cn, initial_start, plot=True):\n",
    "    assert type(cn) == str\n",
    "    \n",
    "    truey = df[cn][initial_start:]\n",
    "    data_length = range(truey.shape[0])\n",
    "    popt, pcov = curve_fit(logistic_function, data_length, truey, bounds=([0,0,0], np.inf), maxfev=2000)\n",
    "    estimated_k, estimated_x_0, maxy = popt\n",
    "    predy = logistic_function(data_length, estimated_k, estimated_x_0, maxy)\n",
    "    mse = np.square(np.subtract(truey,predy)).mean()\n",
    "    \n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "        ax.plot(data_length, predy, 'r--', label='Predict', linewidth=2)\n",
    "        ax.plot(data_length, truey, color='black', label='Actual', linewidth=2)\n",
    "        ax.legend(fontsize='large')\n",
    "        ax.set_xlabel('Logistic Function Error vs Actual | MSE: {}'.format(round(mse, 4)), fontsize=10)\n",
    "        ax.set_ylabel('Values')\n",
    "        ax.yaxis.set_label_position('right')\n",
    "        plt.suptitle('Logistic Function on Inflection Point & Growth Rate', ha='left', x=.015, y=.95, fontsize=20)\n",
    "        ax.set_title('Estimated Growth Rate: {} | Estimated Inflection Point: {} | Estimated Maximum Cases: {}'\\\n",
    "                     .format(round(estimated_k, 4), round(estimated_x_0, 4), round(maxy, 4)))\n",
    "        plt.show()\n",
    "\n",
    "logistic_pred(df=fdata, cn='cases', initial_start=0, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLVX4_WeYrX8"
   },
   "outputs": [],
   "source": [
    "def condition_color_list(columns):\n",
    "    color_list = []\n",
    "    for v in columns:\n",
    "        if v > 0:\n",
    "            color_list.append('#15607a')\n",
    "        else:\n",
    "            color_list.append('#ff483a')\n",
    "    return color_list\n",
    "    \n",
    "def plot_pc(df):\n",
    "    mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "    with plt.style.context(custom_style2): \n",
    "        plt.style.use(['dark_background'])\n",
    "        temp = df.copy()\n",
    "        temp['Total Confirm Percent Change'] = percent_change(temp, 'cases')\n",
    "\n",
    "        color_list = condition_color_list(temp['Total Confirm Percent Change'])\n",
    "        fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "        plt.tight_layout()\n",
    "        ax.bar(x=temp.index, height=temp['Total Confirm Percent Change'], color=color_list)\n",
    "        ax.plot(temp.index, [np.mean(temp['Total Confirm Percent Change'])] * \n",
    "                len(temp.index), label='Mean', linestyle='--', color='#ffb55f')\n",
    "        ax.yaxis.set_label_position('right')\n",
    "        ax.grid(axis='y')\n",
    "        ax.legend(fontsize=7)\n",
    "        ax.set_xlabel('Year')\n",
    "        ax.set_ylabel('Percent Change')\n",
    "        ax.set_title('Covid-19 Confirmed Percent Change', fontweight='bold', loc='left')\n",
    "        ax.set_ylim([-100, 500])\n",
    "        print('Max: {}, Min: {}'.format(temp['Total Confirm Percent Change'].max(), \n",
    "                                        temp['Total Confirm Percent Change'].min()))\n",
    "    \n",
    "# plot_pc(df=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HO2-71-SYrX8"
   },
   "outputs": [],
   "source": [
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.style.use(custom_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4QO47saYrX-"
   },
   "outputs": [],
   "source": [
    "def plot_decomposition(df, cn, window, plot=True):\n",
    "    assert type(cn) == str\n",
    "    \n",
    "    values = df[cn].values.flatten()\n",
    "    rm = np.convolve(values, np.ones(window)/window, mode='valid')\n",
    "    detrended = values[window-1:]/rm\n",
    "    diff = df[cn]-df[cn].shift(1)\n",
    "    \n",
    "    new = df.reset_index().copy()\n",
    "    new['day'] = new['date'].dt.day_name()\n",
    "    countday = new.groupby('day', as_index=False)[[cn]].sum()\n",
    "    \n",
    "    nh = df.copy()\n",
    "    nh['cummax'] = nh[cn].cummax()\n",
    "    nh = nh.drop_duplicates(subset='cummax', keep='first').reset_index()\n",
    "    nh['diff'] = (nh['date'].diff()/ np.timedelta64(1, 'D')).fillna(0)\n",
    "    \n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(3, 1, figsize=(10,10), sharex=True)\n",
    "        index = df.index\n",
    "        axs[0].plot(index, values) \n",
    "        axs[0].set_title('Abstract')\n",
    "        axs[0].set_ylabel('Daily')\n",
    "        axs[0].plot(index[window-1:], rm)\n",
    "        axs[1].plot(index, diff)\n",
    "        axs[1].set_title('Disparity')\n",
    "        axs[1].set_ylabel('Daily')\n",
    "        axs[2].plot(index[window-1:], detrended)\n",
    "        axs[2].set_title('Detrended')\n",
    "        for i in range(3):\n",
    "            axs[i].yaxis.set_label_position('right')\n",
    "        plt.suptitle('Decomposition', ha='left', x=.015, y=1)\n",
    "        fig.autofmt_xdate()\n",
    "        \n",
    "        fig, axs = plt.subplots(2, 1, figsize=(10,8))\n",
    "        axs[0].bar(x=countday['day'], height=countday[cn])\n",
    "        axs[0].set_title('Categorize')\n",
    "        axs[0].set_ylabel('Total')\n",
    "        axs[1].hist(x=new[cn], bins=10)\n",
    "        axs[1].set_title('Distributions')\n",
    "        axs[1].set_ylabel('Counts')\n",
    "        for i in range(2):\n",
    "            axs[i].yaxis.set_label_position('right')\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "        ax.plot(nh['date'], nh['cummax'])\n",
    "        for p in range(len(nh)):\n",
    "            if nh['diff'][p] <= 5.0:\n",
    "                pass\n",
    "            else:\n",
    "                y_ratio = nh['cummax'].max()/275\n",
    "                ax.annotate(str(nh['diff'][p]), (mdates.date2num(nh['date'][p]), p), xytext=(-20, nh['cummax'][p]/y_ratio),  \n",
    "                            textcoords='offset pixels')\n",
    "        ax.set_title('Interval')\n",
    "        ax.set_ylabel('Cumulative Max')\n",
    "        ax.yaxis.set_label_position('right')\n",
    "        \n",
    "plot_decomposition(df=data, cn='cases', window=12, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaJ-vOjVYrX_"
   },
   "source": [
    "### Model Selection and Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1647,
     "status": "ok",
     "timestamp": 1640180981457,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "jgPGSm08YrX_",
    "outputId": "67f29a6a-0870-4705-f376-8a20c68b50d5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(df, cn):\n",
    "    assert type(cn) == str\n",
    "    \n",
    "    # from sklearn import linear_model, preprocessing, model_selection, pipeline, ensemble, tree, kernel_ridge, neighbors, base\n",
    "    \n",
    "    X = df.drop(columns=cn)\n",
    "    y = df[cn]\n",
    "    trainX, testX, trainy, testy = model_selection.train_test_split(X, y, test_size=.8, random_state=7)\n",
    "        \n",
    "    lasso_grid =  model_selection.GridSearchCV(linear_model.Lasso(), \n",
    "                                               param_grid = {'max_iter': [1000, 3000, 5000, 10000, 15000, 30000],\n",
    "                                                             'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "                                                             'normalize': [True, False], \n",
    "                                                             'tol': [0.001, 0.01, 0.1, 1],\n",
    "                                                             'warm_start': [True, False],\n",
    "                                                             'random_state': [7]},\n",
    "                                               cv=10, \n",
    "                                               scoring='neg_mean_squared_error')\n",
    "    \n",
    "    eas_grid = model_selection.GridSearchCV(linear_model.ElasticNet(), \n",
    "                                            param_grid = {'alpha': [1.0, 0.01, 0.1, 0.5, 1], \n",
    "                                                          'l1_ratio': [0.01, 0.5, 0.1, 1], \n",
    "                                                          'normalize': [True, False], \n",
    "                                                          'warm_start': [True, False],\n",
    "                                                          'random_state': [7]},\n",
    "                                            cv=10, \n",
    "                                            scoring='neg_mean_squared_error')\n",
    "    \n",
    "    gbr_grid = model_selection.GridSearchCV(ensemble.GradientBoostingRegressor(), \n",
    "                                            param_grid = {'learning_rate': [0.1, 0.5, 1],\n",
    "                                                          'n_estimators': [100, 110, 120],\n",
    "                                                          'min_samples_split': [2, 4, 8], \n",
    "                                                          'min_samples_leaf': [1, 2, 4],\n",
    "                                                          'max_depth': [3, 6, 9],\n",
    "                                                          'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                                                          'warm_start': [True, False],\n",
    "                                                          'random_state': [7]},\n",
    "                                            cv=10, \n",
    "                                            scoring='neg_mean_squared_error')\n",
    "    \n",
    "    rfr_grid = model_selection.GridSearchCV(ensemble.RandomForestRegressor(), \n",
    "                                            param_grid = {'n_estimators': [110, 120], \n",
    "                                                          'min_samples_split': [2],\n",
    "                                                          'min_samples_leaf': [1],\n",
    "                                                          'max_depth': [6, 9],\n",
    "                                                          'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                                                          'warm_start': [True, False],\n",
    "                                                          'random_state': [7]},\n",
    "                                            cv=10, \n",
    "                                            scoring='neg_mean_squared_error')\n",
    "    \n",
    "    lgb_grid = model_selection.GridSearchCV(lgb.LGBMRegressor(),\n",
    "                                            param_grid = {'num_leaves': [10, 31, 50, 75, 150],\n",
    "                                                          'max_depth': [25, 50, 75],\n",
    "                                                          'learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "                                                          'n_estimators': [100, 125, 150],\n",
    "                                                          'random_state': [7]},\n",
    "                                            cv=10, \n",
    "                                            scoring='neg_mean_squared_error')\n",
    "    \n",
    "    xgb_grid = model_selection.GridSearchCV(xgb.XGBRegressor(),\n",
    "                                            param_grid = {'n_estimators': [100, 125, 150],\n",
    "                                                          'max_depth': [25, 50, 75],\n",
    "                                                          'learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "                                                          'min_child_weight': [1,3,5],\n",
    "                                                          'random_state': [7]},\n",
    "                                            cv=10,\n",
    "                                            scoring='neg_mean_squared_error')\n",
    "    \n",
    "    ridge_grid = model_selection.GridSearchCV(linear_model.Ridge(),\n",
    "                                              param_grid = {'max_iter': [1000, 3000, 5000, 10000, 15000, 30000],\n",
    "                                                            'normalize': [True, False],\n",
    "                                                            'tol': [0.001, 0.01, 0.1, 1],\n",
    "                                                            'random_state': [7]},\n",
    "                                              cv=10,\n",
    "                                              scoring='neg_mean_squared_error')\n",
    "    \n",
    "    sgd_grid = model_selection.GridSearchCV(linear_model.SGDRegressor(), \n",
    "                                            param_grid = {'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "                                                          'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "                                                          'max_iter': [1000, 3000, 5000, 10000, 15000, 30000],\n",
    "                                                          'tol': [0.001, 0.01, 0.1, 1], \n",
    "                                                          'early_stopping': [True], \n",
    "                                                          'warm_start': [True, False],\n",
    "                                                          'random_state': [7]},\n",
    "                                            cv=10,\n",
    "                                            scoring='neg_mean_squared_error')\n",
    "    \n",
    "    dt_grid = model_selection.GridSearchCV(tree.DecisionTreeRegressor(),\n",
    "                                           param_grid = {'min_samples_split': [2, 4, 5],\n",
    "                                                         'min_samples_leaf': [1, 2, 3],\n",
    "                                                         'max_features': ['auto', 'sqrt', 'log2'], \n",
    "                                                         'max_leaf_nodes': [10, 15, 20, 30],\n",
    "                                                         'random_state': [7]},\n",
    "                                           cv=10,\n",
    "                                           scoring='neg_mean_squared_error')\n",
    "    \n",
    "    ada_grid = model_selection.GridSearchCV(ensemble.AdaBoostRegressor(),\n",
    "                                               param_grid = {'n_estimators': [25, 50, 75, 100],\n",
    "                                                             'learning_rate': [0.01, 0.1, 1],\n",
    "                                                             'loss': ['linear', 'square', 'exponential'], \n",
    "                                                             'random_state': [7]},\n",
    "                                               cv=10,\n",
    "                                               scoring='neg_mean_squared_error')\n",
    "    \n",
    "    kerid_grid = model_selection.GridSearchCV(kernel_ridge.KernelRidge(),\n",
    "                                            param_grid = {'alpha': [0.01, 0.1, 1], \n",
    "                                                          'gamma': [0.001, 0.01, 1]})\n",
    "    \n",
    "    kn_grid = model_selection.GridSearchCV(neighbors.KNeighborsRegressor(), \n",
    "                                           param_grid = {'n_neighbors': [5, 10, 15], \n",
    "                                                         'weights': ['uniform', 'distance'],\n",
    "                                                         'leaf_size': [5, 10, 20, 30, 40, 50]})\n",
    "    \n",
    "    \n",
    "    # for m in [lasso_grid, eas_grid, gbr_grid, rfr_grid, lgb_grid, xgb_grid, \n",
    "    #           ridge_grid, sgd_grid, dt_grid, ada_grid, kerid_grid, kn_grid]:\n",
    "    #     m.fit(trainX, trainy)\n",
    "    #     print(m.best_params_)\n",
    "    \n",
    "    # Best params\n",
    "    lasrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   linear_model.Lasso(alpha=0.0001, max_iter=10000, normalize=False, \n",
    "                                                      tol=0.001, warm_start=True, random_state=7))\n",
    "    \n",
    "    easrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   linear_model.ElasticNet(alpha=1.0, l1_ratio=0.01, normalize=False, \n",
    "                                                           warm_start=True, random_state=7))\n",
    "    \n",
    "    gbrrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   ensemble.GradientBoostingRegressor(max_depth=6, max_features='log2', \n",
    "                                                                      min_samples_leaf=1, min_samples_split=2, \n",
    "                                                                      n_estimators=110, warm_start=True, random_state=7))\n",
    "    \n",
    "    rfrss = pipeline.make_pipeline(preprocessing.StandardScaler(), \n",
    "                                   ensemble.RandomForestRegressor(max_depth=6, max_features='log2', \n",
    "                                                                  min_samples_leaf=1, min_samples_split=2, \n",
    "                                                                  n_estimators=110, warm_start=True, random_state=7))\n",
    "    \n",
    "    lgbmm = pipeline.make_pipeline(preprocessing.MinMaxScaler(), \n",
    "                                   lgb.LGBMRegressor(learning_rate=0.1, max_depth=25, n_estimators=100, \n",
    "                                                     num_leaves=10, random_state=7))\n",
    "    \n",
    "    xgbrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   xgb.XGBRegressor(learning_rate=0.1, max_depth=25, min_child_weight=3, \n",
    "                                                    n_estimators=100, random_state=7))\n",
    "    \n",
    "    ridrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   linear_model.Ridge(max_iter=1000, normalize=False, tol=0.001, random_state=7))\n",
    "    \n",
    "    sgdrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   linear_model.SGDRegressor(alpha=0.1, early_stopping=True, max_iter=1000, \n",
    "                                                             penalty='elasticnet', tol=0.001, warm_start=True, random_state=7))\n",
    "    \n",
    "    dtrrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   tree.DecisionTreeRegressor(max_features='sqrt', max_leaf_nodes=20, \n",
    "                                                              min_samples_leaf=1, min_samples_split=2, random_state=7))\n",
    "    \n",
    "    adars = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   ensemble.AdaBoostRegressor(learning_rate=1, loss='square', n_estimators=25, random_state=7))\n",
    "    \n",
    "    kerrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   kernel_ridge.KernelRidge(alpha=0.1, gamma=0.001))\n",
    "    \n",
    "    knrrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   neighbors.KNeighborsRegressor(leaf_size=5, n_neighbors=5, weights='distance'))\n",
    "\n",
    "    model_namelist = ['Lasso', 'ElasticNet', 'GradientBoosting', 'RandomForest', 'LightGBM', 'XGBoost', 'Ridge', 'SGD',\\\n",
    "                      'DecisionTree', 'AdaBoost', 'KernelRidge', 'KNeighbors']\n",
    "    \n",
    "    model_list = [lasrs, easrs, gbrrs, rfrss, lgbmm, xgbrs, ridrs, sgdrs, dtrrs, adars, kerrs, knrrs]\n",
    "    for n, m in zip(model_namelist, model_list):\n",
    "        m.fit(trainX, trainy)\n",
    "        predicted = m.predict(testX)\n",
    "        mse = np.mean((testy - predicted)**2)\n",
    "        print('Model: {0:20} MSE: {1}'.format(n, round(mse, 2)))\n",
    "\n",
    "hyperparameter_tuning(df=fdata, cn='cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1640180984051,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "HPC3jXlqYrYB",
    "outputId": "b298270a-b529-4ba4-fe03-80908946c136"
   },
   "outputs": [],
   "source": [
    "def ml_feature_selection(df, cn, tnf):\n",
    "    assert type(cn) == str\n",
    "    \n",
    "    X = df.drop(columns=cn); y = df[cn]\n",
    "    trainX, testX, trainy, testy = model_selection.train_test_split(X, y, test_size=.8, random_state=7)\n",
    "    \n",
    "    randomforest = ensemble.RandomForestRegressor(max_depth=6, max_features='log2', min_samples_leaf=1, min_samples_split=2, \n",
    "                                                  n_estimators=110, warm_start=True, random_state=7)\n",
    "    randomforest.fit(trainX, trainy)\n",
    "    fi = randomforest.feature_importances_\n",
    "    indices = np.argsort(fi)\n",
    "    columns = trainX.columns\n",
    "    std = np.std([tree.feature_importances_ for tree in randomforest.estimators_])\n",
    "    print('Explonatory Variables: {} \\nResponse Variables: {}\\n'.format(trainX.columns.tolist(), cn))\n",
    "    print('Feature in Order: ')\n",
    "    for f in range(trainX.shape[1]):\n",
    "        print('%d. Feature_no: %d Feature_name: %s (%f)' % (f+1, indices[f], columns[indices[f]], fi[indices[f]]))\n",
    "\n",
    "    feature_selected = columns[indices].tolist()\n",
    "    feature_selected.insert(0, cn)\n",
    "    return feature_selected\n",
    "\n",
    "feature_selected = ml_feature_selection(df=fdata, cn='cases', tnf=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQAWUnb2YrYB"
   },
   "source": [
    "### Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2618,
     "status": "ok",
     "timestamp": 1640180989244,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "pHbE9AIGYrYC",
    "outputId": "71ce2177-e661-4cdc-8726-71b023e20080"
   },
   "outputs": [],
   "source": [
    "class AveragingModels(base.BaseEstimator, base.RegressorMixin, base.TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [base.clone(x) for x in self.models]\n",
    "        \n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_])\n",
    "        return np.mean(predictions, axis=1)  \n",
    "\n",
    "def CombinedModels(df, cn):\n",
    "    assert type(cn) == str\n",
    "    \n",
    "    lasrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   linear_model.Lasso(alpha=0.0001, max_iter=10000, normalize=False, tol=0.001, \n",
    "                                                      warm_start=True, random_state=7))\n",
    "    \n",
    "    gbrrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   ensemble.GradientBoostingRegressor(max_depth=6, max_features='log2', min_samples_leaf=1, \n",
    "                                                                      min_samples_split=2, n_estimators=110, \n",
    "                                                                      warm_start=True, random_state=7))\n",
    "    \n",
    "    rfrss = pipeline.make_pipeline(preprocessing.StandardScaler(), \n",
    "                                   ensemble.RandomForestRegressor(max_depth=6, max_features='log2', min_samples_leaf=1, \n",
    "                                                                  min_samples_split=2, n_estimators=110, \n",
    "                                                                  warm_start=True, random_state=7))\n",
    "    \n",
    "    ridrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   linear_model.Ridge(max_iter=1000, normalize=False, tol=0.001, random_state=7))\n",
    "    \n",
    "    knrrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   neighbors.KNeighborsRegressor(leaf_size=5, n_neighbors=5, weights='distance'))\n",
    "    \n",
    "    kerrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   kernel_ridge.KernelRidge(alpha=0.1, gamma=0.001))\n",
    "    \n",
    "    model_list = [lasrs, gbrrs, rfrss]\n",
    "    \n",
    "    X = df.drop(columns=cn)\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    y = df[cn]\n",
    "    trainX, testX, trainy, testy = model_selection.train_test_split(X, y, test_size=.8, random_state=7)\n",
    "    \n",
    "    for m in model_list:\n",
    "        m.fit(trainX, trainy)\n",
    "        pred = m.predict(testX)\n",
    "        \n",
    "    # Stack Model\n",
    "    stacked = StackingCVRegressor(regressors=(lasrs, gbrrs, rfrss, ridrs, knrrs, kerrs), meta_regressor=lasrs, \n",
    "                                  use_features_in_secondary=True, random_state=7)\n",
    "    stacked.fit(trainX, trainy)\n",
    "    stacked_predicted = stacked.predict(testX)\n",
    "    stacked_mse = np.square(np.subtract(testy, stacked_predicted)).mean()\n",
    "    print('Stacked MSE:', round(stacked_mse, 2))\n",
    "    \n",
    "    # Blended Model\n",
    "    blended_model = ((0.2 * model_list[0].predict(testX)) + \n",
    "                     (0.40 * model_list[1].predict(testX)) + \n",
    "                     (0.40 * model_list[2].predict(testX)))\n",
    "    \n",
    "    blended_mse = np.square(np.subtract(testy, blended_model)).mean()\n",
    "    print('Blended MSE:', round(blended_mse, 2))\n",
    "    \n",
    "    # Average Model\n",
    "    averaged_models = AveragingModels(models=(model_list))\n",
    "    averaged_models.fit(trainX, trainy)\n",
    "    averaged_predicted = averaged_models.predict(testX)\n",
    "    ageraged_mse = np.square(np.subtract(testy, averaged_predicted)).mean()\n",
    "    print('Averaged MSE:', round(ageraged_mse, 2))\n",
    "    \n",
    "    return stacked\n",
    "    \n",
    "stacked_model = CombinedModels(df=fdata, cn='cases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3O3aObx3YrYC"
   },
   "source": [
    "Average Model: Lasso, GradientBoosting, RandomForest <br>\n",
    "Stacked Model: Lasso, GradientBoosting, RandomForest, Ridge, Kneighbors, KernelRidge <br>\n",
    "Blended Model: 0.2(Lasso), 0.4(GradientBoosting), 0.4(RandomForest) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSGLBRkVYrYC"
   },
   "source": [
    "### Forecast with Randomforest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4ddDf3KYrYC"
   },
   "outputs": [],
   "source": [
    "def randomforest_model(df, cn, lag, forecast, plot):\n",
    "    assert type(cn) == str\n",
    "    \"\"\"\n",
    "    Timeseries to supervised and forecast Lasso\n",
    "    Parameters:\n",
    "    -----------\n",
    "        df: pandas.DataFrame\n",
    "            The dataframe with explanatory and response variables \n",
    "        cn: str\n",
    "            The columns variables to forecast\n",
    "        lag: int\n",
    "            The preceding lag for time series data construction\n",
    "        forecast: integer\n",
    "            The time horizon to forecast in days\n",
    "        plot: boolean, optional\n",
    "            Visualize plot\n",
    "    -----------\n",
    "    Returns:\n",
    "        - \n",
    "    \"\"\"\n",
    "    # Preprocessing    \n",
    "    trainX = df.sample(frac=.8, random_state=7); testX = df.drop(trainX.index)\n",
    "    trainy, testy = trainX.pop(cn), testX.pop(cn)\n",
    "    tf = trainX.columns.tolist()\n",
    "    \n",
    "    r = ensemble.RandomForestRegressor(max_depth=6, max_features='log2', min_samples_leaf=1, min_samples_split=2, \n",
    "                                       n_estimators=110, warm_start=True, random_state=7)\n",
    "    r.fit(trainX, trainy)\n",
    "    pred = r.predict(testX)\n",
    "    \n",
    "    # Training\n",
    "    model_list = list() \n",
    "    for x in range(len(tf)):\n",
    "        data = df[tf[x]].tolist()\n",
    "        n_vars = 1 if type(data) is list else data.shape[1]\n",
    "        temp = pd.DataFrame(data)\n",
    "        cols, names = list(), list()\n",
    "\n",
    "        for i in range(lag, 0, -1): # Input sequence (t-n, ... t-1)\n",
    "            cols.append(temp.shift(i))\n",
    "            names += [('%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        cols.append(temp.shift(-1)) # Current forecast t\n",
    "        names += [('%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        \n",
    "        new_df = pd.concat(cols, axis=1)\n",
    "        new_df.columns = names\n",
    "        new_df.dropna(inplace=True) \n",
    "        new_df = new_df.add_prefix(str(tf[x])+' ')\n",
    "        \n",
    "        XX, yy = new_df.iloc[:, :lag], new_df.iloc[:, lag:lag+1]\n",
    "        trainXX = XX.sample(frac=.8, random_state=7); testXX = XX.drop(trainXX.index)\n",
    "        trainyy = yy.sample(frac=.8, random_state=7); testyy = yy.drop(trainyy.index)\n",
    "        model = ensemble.RandomForestRegressor(max_depth=6, max_features='log2', min_samples_leaf=1, min_samples_split=2, \n",
    "                                               n_estimators=110, warm_start=True, random_state=7)\n",
    "        model.fit(trainXX, trainyy)\n",
    "        model_list.append(model)\n",
    "        # print(np.sqrt(np.mean(testyy.values.flatten()-model.predict(testXX))**2))\n",
    "             \n",
    "    # Forecast\n",
    "    result = df[tf].iloc[-lag:, :].copy(); # display(result)\n",
    "    result['Forecast'] = 0\n",
    "    for _ in range(forecast):\n",
    "        temp_result = list()\n",
    "        for x in range(len(model_list)):\n",
    "            temp_result.append(int(model_list[x].predict(result.iloc[-lag:, x].values.reshape(1, -1))))\n",
    "        temp_result.append(int(r.predict(np.array(temp_result).reshape(1, -1))))\n",
    "        result = result.append(pd.Series(temp_result, index=result.columns), ignore_index=True)\n",
    "    \n",
    "    moving_average_window = 7\n",
    "    result['Forecast Moving Average'] = result['Forecast'].rolling(window=moving_average_window).mean()\n",
    "    \n",
    "    # Output\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(df[cn])\n",
    "        plt.plot(pd.Series(result['Forecast'][-forecast:].tolist(), \n",
    "                           index=np.arange(int(df.index[-1:].values[0]), \n",
    "                                           int(df.index[-1:].values[0])+forecast)))\n",
    "        plt.plot(pd.Series(result['Forecast Moving Average'][-forecast+moving_average_window:].tolist(), \n",
    "                           index=np.arange(int(df.index[-1:].values[0])+moving_average_window, \n",
    "                                           int(df.index[-1:].values[0])+forecast)))\n",
    "        plt.legend(['Actual', 'Forecast', 'Forecast Moving Average'], ncol=5, loc='upper left')\n",
    "        plt.title('Forecast: {} | Lag: {}'.format(forecast, lag), color='#787878')\n",
    "        plt.suptitle('Projection: {} Forward Forecast'.format(cn), ha='left', x=.015, y=.95, fontsize=20)\n",
    "        plt.figtext(0.85, 0.9, 'Random Forest Model', ha='left')\n",
    "        ax.set_xlabel('Range'); ax.set_ylabel('Values'); ax.yaxis.set_label_position('right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 76515,
     "status": "ok",
     "timestamp": 1640181071737,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "vVGbyW0sYrYD",
    "outputId": "7651fba8-9180-4d00-c068-07828725fea3"
   },
   "outputs": [],
   "source": [
    "randomforest_model(df=fdata, cn='cases', lag=7, forecast=90, plot=True)\n",
    "randomforest_model(df=fdata, cn='cases', lag=14, forecast=90, plot=True)\n",
    "randomforest_model(df=fdata, cn='cases', lag=30, forecast=90, plot=True)\n",
    "randomforest_model(df=fdata, cn='cases', lag=30, forecast=120, plot=True)\n",
    "randomforest_model(df=fdata, cn='cases', lag=60, forecast=120, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0nGSwiyYrYD"
   },
   "source": [
    "### Forecast with Lasso Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBMp3CsDYrYD"
   },
   "outputs": [],
   "source": [
    "def lasso_model(df, cn, ntf, lag, forecast, plot):\n",
    "    assert type(cn) == str\n",
    "    \"\"\"\n",
    "    Timeseries to supervised and forecast Lasso\n",
    "    Parameters:\n",
    "    -----------\n",
    "        df: pandas.DataFrame\n",
    "            The dataframe with explanatory and response variables \n",
    "        cn: str\n",
    "            The columns variables to forecast\n",
    "        ntf: int\n",
    "            The number of top feature to select rank by lasso coefficient\n",
    "        lag: int\n",
    "            The preceding lag for time series data construction\n",
    "        forecast: integer\n",
    "            The time horizon to forecast in days\n",
    "        plot: boolean, optional\n",
    "            Visualize plot\n",
    "    -----------\n",
    "    Returns:\n",
    "        - \n",
    "    \"\"\"\n",
    "    # Preprocessing    \n",
    "    trainX = df.sample(frac=.8, random_state=7); testX = df.drop(trainX.index)\n",
    "    trainy, testy = trainX.pop(cn), testX.pop(cn)\n",
    "    \n",
    "    l = linear_model.Lasso(alpha=0.0001, max_iter=10000, normalize=False, tol=0.001, warm_start=True, random_state=7) \n",
    "    l.fit(trainX, trainy)\n",
    "    pred = l.predict(testX)\n",
    "    feature_df = pd.DataFrame({'Feature': df.drop(columns=cn).columns, 'Coefficient': l.coef_})\n",
    "    tf = feature_df.sort_values(by='Coefficient', ascending=False).iloc[:ntf, 0].to_list()\n",
    "\n",
    "    # Feature selection\n",
    "    lasso = linear_model.Lasso(alpha=0.0001, max_iter=10000, normalize=False, tol=0.001, warm_start=True, random_state=7)\n",
    "    lasso.fit(trainX[tf], trainy)\n",
    "    apred = lasso.predict(testX[tf])\n",
    "    \n",
    "    # Training\n",
    "    s_feature_model = list() \n",
    "    for x in range(len(tf)):\n",
    "        data = df[tf[x]].tolist()\n",
    "        n_vars = 1 if type(data) is list else data.shape[1]\n",
    "        temp = pd.DataFrame(data)\n",
    "        cols, names = list(), list()\n",
    "\n",
    "        for i in range(lag, 0, -1): # Input sequence (t-n, ... t-1)\n",
    "            cols.append(temp.shift(i))\n",
    "            names += [('%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        cols.append(temp.shift(-1)) # Current forecast t\n",
    "        names += [('%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        \n",
    "        new_df = pd.concat(cols, axis=1)\n",
    "        new_df.columns = names\n",
    "        new_df.dropna(inplace=True) \n",
    "        new_df = new_df.add_prefix(str(tf[x])+' ')\n",
    "        \n",
    "        XX, yy = new_df.iloc[:, :lag], new_df.iloc[:, lag:lag+1]\n",
    "        trainXX = XX.sample(frac=.8, random_state=7); testXX = XX.drop(trainXX.index)\n",
    "        trainyy = yy.sample(frac=.8, random_state=7); testyy = yy.drop(trainyy.index)\n",
    "        model = linear_model.Lasso(alpha=0.0001, max_iter=10000, normalize=False, tol=0.001, warm_start=True, random_state=7)\n",
    "        model.fit(trainXX, trainyy)\n",
    "        s_feature_model.append(model)\n",
    "        # print(np.sqrt(np.mean(testyy.values.flatten()-model.predict(testXX))**2))\n",
    "             \n",
    "    # Forecast\n",
    "    result = df[tf].iloc[-lag:, :].copy(); # display(result)\n",
    "    result['Forecast'] = 0\n",
    "    for _ in range(forecast):\n",
    "        temp_result = list()\n",
    "        for x in range(len(s_feature_model)):\n",
    "            temp_result.append(int(s_feature_model[x].predict(result.iloc[-lag:, x].values.reshape(1, -1))))\n",
    "        temp_result.append(int(lasso.predict(np.array(temp_result).reshape(1, -1))))\n",
    "        result = result.append(pd.Series(temp_result, index=result.columns), ignore_index=True)\n",
    "    \n",
    "    moving_average_window = 7\n",
    "    result['Forecast Moving Average'] = result['Forecast'].rolling(window=moving_average_window).mean()\n",
    "    \n",
    "    # Output\n",
    "    # feature_df.plot.bar(x='Feature', y='Coefficient')\n",
    "    # print('Without feature Selection: {}'.format(np.sqrt(np.mean(testy-pred)**2)))\n",
    "    # print('With feature selection: {}'.format(np.sqrt(np.mean(testy-apred)**2)))\n",
    "    # print('Forecast: {} | Lag: {}'.format(forecast, lag))\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(df[cn])\n",
    "        plt.plot(pd.Series(result['Forecast'][-forecast:].tolist(), \n",
    "                           index=np.arange(int(df.index[-1:].values[0]), \n",
    "                                           int(df.index[-1:].values[0])+forecast)))\n",
    "        plt.plot(pd.Series(result['Forecast Moving Average'][-forecast+moving_average_window:].tolist(), \n",
    "                           index=np.arange(int(df.index[-1:].values[0])+moving_average_window, \n",
    "                                           int(df.index[-1:].values[0])+forecast)))\n",
    "        plt.legend(['Actual', 'Forecast', 'Forecast Moving Average'], ncol=5, loc='upper left')\n",
    "        plt.title('Feature Selected: {} | Forecast: {} | Lag: {}'.format(ntf, forecast, lag), color='#787878')\n",
    "        plt.suptitle('Projection: {} Forward Forecast'.format(cn), ha='left', x=.015, y=.95, fontsize=20)\n",
    "        plt.figtext(0.85, 0.9, 'Lasso Model', ha='left')\n",
    "        ax.set_xlabel('Range'); ax.set_ylabel('Values'); ax.yaxis.set_label_position('right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 947
    },
    "executionInfo": {
     "elapsed": 2127,
     "status": "ok",
     "timestamp": 1640181075626,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "7kXR_pVfYrYD",
    "outputId": "cea2293e-debb-43b2-a40f-50cb93700878"
   },
   "outputs": [],
   "source": [
    "lasso_model(df=fdata, cn='cases', ntf=3, lag=7, forecast=90, plot=True)\n",
    "lasso_model(df=fdata, cn='cases', ntf=3, lag=30, forecast=90, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOt5FhVnYrYE"
   },
   "source": [
    "### Forecast with Stacked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHvWvIGoYrYE"
   },
   "outputs": [],
   "source": [
    "def stacked_model(df, cn, lag, forecast, plot):\n",
    "    assert type(cn) == str\n",
    "    \"\"\"\n",
    "    Timeseries to supervised and forecast Lasso\n",
    "    Parameters:\n",
    "    -----------\n",
    "        df: pandas.DataFrame\n",
    "            The dataframe with explanatory and response variables \n",
    "        cn: str\n",
    "            The columns variables to forecast\n",
    "        lag: int\n",
    "            The preceding lag for time series data construction\n",
    "        forecast: integer\n",
    "            The time horizon to forecast in days\n",
    "        plot: boolean, optional\n",
    "            Visualize plot\n",
    "    -----------\n",
    "    Returns:\n",
    "        - \n",
    "    \"\"\"\n",
    "    # Preprocessing    \n",
    "    trainX = df.sample(frac=.8, random_state=7); testX = df.drop(trainX.index)\n",
    "    trainy, testy = trainX.pop(cn), testX.pop(cn)\n",
    "    tf = trainX.columns.tolist()\n",
    "    \n",
    "    lasrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   linear_model.Lasso(alpha=0.0001, max_iter=10000, normalize=False, \n",
    "                                                      tol=0.001, warm_start=True, random_state=7))\n",
    "    \n",
    "    gbrrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   ensemble.GradientBoostingRegressor(max_depth=6, max_features='log2', \n",
    "                                                                      min_samples_leaf=1, min_samples_split=2, \n",
    "                                                                      n_estimators=110, warm_start=True, random_state=7))\n",
    "    \n",
    "    rfrss = pipeline.make_pipeline(preprocessing.StandardScaler(), \n",
    "                                   ensemble.RandomForestRegressor(max_depth=6, max_features='log2', min_samples_leaf=1, \n",
    "                                                                  min_samples_split=2, n_estimators=110, warm_start=True, \n",
    "                                                                  random_state=7))\n",
    "    ridrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   linear_model.Ridge(max_iter=1000, normalize=False, tol=0.001, random_state=7))\n",
    "    \n",
    "    knrrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   neighbors.KNeighborsRegressor(leaf_size=5, n_neighbors=5, weights='distance'))\n",
    "    \n",
    "    kerrs = pipeline.make_pipeline(preprocessing.RobustScaler(), \n",
    "                                   kernel_ridge.KernelRidge(alpha=0.1, gamma=0.001))\n",
    "    \n",
    "    stacked_model = StackingCVRegressor(regressors=(lasrs, gbrrs, rfrss, ridrs, knrrs, kerrs), \n",
    "                                        meta_regressor=lasrs, use_features_in_secondary=True, random_state=7)\n",
    "    \n",
    "    stacked_model.fit(trainX, trainy)\n",
    "    pred = stacked_model.predict(testX)\n",
    "    \n",
    "    # Training\n",
    "    model_list = list() \n",
    "    for x in range(len(tf)):\n",
    "        data = df[tf[x]].tolist()\n",
    "        n_vars = 1 if type(data) is list else data.shape[1]\n",
    "        temp = pd.DataFrame(data)\n",
    "        cols, names = list(), list()\n",
    "\n",
    "        for i in range(lag, 0, -1): # Input sequence (t-n, ... t-1)\n",
    "            cols.append(temp.shift(i))\n",
    "            names += [('%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        cols.append(temp.shift(-1)) # Current forecast t\n",
    "        names += [('%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        \n",
    "        new_df = pd.concat(cols, axis=1)\n",
    "        new_df.columns = names\n",
    "        new_df.dropna(inplace=True) \n",
    "        new_df = new_df.add_prefix(str(tf[x])+' ')\n",
    "        \n",
    "        XX, yy = new_df.iloc[:, :lag], new_df.iloc[:, lag:lag+1]\n",
    "        trainXX = XX.sample(frac=.8, random_state=7); testXX = XX.drop(trainXX.index)\n",
    "        trainyy = yy.sample(frac=.8, random_state=7); testyy = yy.drop(trainyy.index)\n",
    "        model = StackingCVRegressor(regressors=(lasrs, gbrrs, rfrss, ridrs, knrrs, kerrs), \n",
    "                                    meta_regressor=lasrs, use_features_in_secondary=True, random_state=7)\n",
    "        model.fit(trainXX, trainyy)\n",
    "        model_list.append(model)\n",
    "        # print(np.sqrt(np.mean(testyy.values.flatten()-model.predict(testXX))**2))\n",
    "             \n",
    "    # Forecast\n",
    "    result = df[tf].iloc[-lag:, :].copy(); # display(result)\n",
    "    result['Forecast'] = 0\n",
    "    for _ in range(forecast):\n",
    "        temp_result = list()\n",
    "        for x in range(len(model_list)):\n",
    "            temp_result.append(int(model_list[x].predict(result.iloc[-lag:, x].values.reshape(1, -1))))\n",
    "        temp_result.append(int(stacked_model.predict(np.array(temp_result).reshape(1, -1))))\n",
    "        result = result.append(pd.Series(temp_result, index=result.columns), ignore_index=True)\n",
    "    \n",
    "    moving_average_window = 7\n",
    "    result['Forecast Moving Average'] = result['Forecast'].rolling(window=moving_average_window).mean()\n",
    "    \n",
    "    # Output\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(df[cn])\n",
    "        plt.plot(pd.Series(result['Forecast'][-forecast:].tolist(), \n",
    "                           index=np.arange(int(df.index[-1:].values[0]), \n",
    "                                           int(df.index[-1:].values[0])+forecast)))\n",
    "        plt.plot(pd.Series(result['Forecast Moving Average'][-forecast+moving_average_window:].tolist(), \n",
    "                           index=np.arange(int(df.index[-1:].values[0])+moving_average_window, \n",
    "                                           int(df.index[-1:].values[0])+forecast)))\n",
    "        plt.legend(['Actual', 'Forecast', 'Forecast Moving Average'], ncol=5, loc='upper left')\n",
    "        plt.title('Forecast: {} | Lag: {}'.format(forecast, lag), color='#787878')\n",
    "        plt.suptitle('Projection: {} Forward Forecast'.format(cn), ha='left', x=.015, y=.95, fontsize=20)\n",
    "        plt.figtext(0.85, 0.9, 'Stacked Model', ha='left')\n",
    "        ax.set_xlabel('Range'); ax.set_ylabel('Values'); ax.yaxis.set_label_position('right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 947
    },
    "executionInfo": {
     "elapsed": 46241,
     "status": "ok",
     "timestamp": 1640181143160,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "eHViYf8mYrYE",
    "outputId": "3b1f941b-0e1b-4e61-e179-91f2b31b97be"
   },
   "outputs": [],
   "source": [
    "number_of_features = 3\n",
    "stacked_model(df=fdata, cn='cases', lag=14, forecast=90, plot=True)\n",
    "stacked_model(df=fdata[feature_selected[:number_of_features+1]], cn='cases', lag=14, forecast=90, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlvWuhzRYrYE"
   },
   "source": [
    "### Deep Learning Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105401,
     "status": "ok",
     "timestamp": 1640181248555,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "-7WA2W8wYrYE",
    "outputId": "5341ca59-abe6-4c08-9f08-b0452df8a048"
   },
   "outputs": [],
   "source": [
    "def definetuner(hp):\n",
    "    lstm = tf.keras.Sequential()\n",
    "    lstm.add(\n",
    "        tf.keras.layers.LSTM(\n",
    "            units=hp.Int('units', min_value=50, max_value=200, step=50), return_sequences=True\n",
    "        )\n",
    "    )\n",
    "    lstm.add(\n",
    "        tf.keras.layers.LSTM(\n",
    "            units=hp.Int('units', min_value=50, max_value=200, step=50)\n",
    "        )\n",
    "    )\n",
    "    lstm.add(tf.keras.layers.Dense(units=1))\n",
    "    learning_rate = hp.Float('lr', min_value=1e-4, max_value=1e-2)\n",
    "    lstm.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "        loss='mean_squared_error',\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()]\n",
    "    )\n",
    "    return lstm\n",
    "    \n",
    "\n",
    "def tuninglstm(df, cn, window, forecast):\n",
    "    \n",
    "    data = df.filter([cn]).values\n",
    "    nmin, nmax = data.min(), data.max()\n",
    "    scaled_data = ((df[cn]-nmin)/(nmax-nmin)).values.reshape(-1,1)\n",
    "    split = round(0.8*len(data)) \n",
    "    train = scaled_data[:split]\n",
    "    trainX, trainy = [], []\n",
    "    for i in range(window, len(train)):\n",
    "        trainX.append(train[i-window:i, 0])\n",
    "        trainy.append(train[i, 0])\n",
    "    trainX, trainy = np.array(trainX), np.array(trainy)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "    \n",
    "    tuner = kt.RandomSearch(\n",
    "        hypermodel=definetuner,\n",
    "        objective=kt.Objective('mean_squared_error', direction='min'),\n",
    "        max_trials=3,\n",
    "        executions_per_trial=2,\n",
    "        overwrite=True,\n",
    "        directory='tuning_result',\n",
    "        project_name='lstm_tuning',\n",
    "    )\n",
    "    \n",
    "    tuner.search(trainX, trainy, epochs=10)\n",
    "    tuner.search_space_summary()\n",
    "    tuner.results_summary()\n",
    "    \n",
    "tuninglstm(df=data, cn='cases', window=45, forecast=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MlGy_smYrYF"
   },
   "source": [
    "### Forecast with Long Short Term Memory Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1p06-FeiYrYF"
   },
   "outputs": [],
   "source": [
    "def lstm_model(df, cn, window, forecast, epochs, bs, n_samples=1, seed=True, plot=True):\n",
    "    assert type(cn) == str\n",
    "    \"\"\"\n",
    "    Train and forecast LSTM. \n",
    "    Parameters:\n",
    "    -----------\n",
    "        df: pandas.DataFrame\n",
    "            The Dataframe with datetime.date as index with explanatory and response variables  \n",
    "        cn: str\n",
    "            The columns variables to forecast\n",
    "        window: int\n",
    "            The preceding window size for time series data construction\n",
    "        forecast: int\n",
    "            The time horizon to forecast\n",
    "        epochs: int\n",
    "            The number of training epochs\n",
    "        bs: int\n",
    "            The number of training batch_size\n",
    "        n_samples: int, default=1\n",
    "            The number of model created for computing confidence interval\n",
    "        plot: boolean, default=True, optional\n",
    "            Visualize plot\n",
    "    ----------\n",
    "    Returns: \n",
    "        - \n",
    "    \"\"\"\n",
    "    # Preprocessing                                                       \n",
    "    data = df.filter([cn]).values\n",
    "    nmin, nmax = data.min(), data.max()\n",
    "    scaled_data = ((df[cn]-nmin)/(nmax-nmin)).values.reshape(-1,1)\n",
    "    split = round(0.8*len(data)) \n",
    "    train = scaled_data[:split]\n",
    "    trainX, trainy = [], []\n",
    "    for i in range(window, len(train)):\n",
    "        trainX.append(train[i-window:i, 0])\n",
    "        trainy.append(train[i, 0])\n",
    "    trainX, trainy = np.array(trainX), np.array(trainy)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "    \n",
    "    tf.random.set_seed(7) # set seed for reproducibility\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.003, patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Training\n",
    "    lstm_input = tf.keras.layers.Input(shape=(trainX.shape[1], 1))\n",
    "    lstm_1 = tf.keras.layers.LSTM(units=50, return_sequences=True)(lstm_input)\n",
    "    lstm_2 = tf.keras.layers.LSTM(units=150)(lstm_1)\n",
    "    lstm_output = tf.keras.layers.Dense(units=1)(lstm_2)\n",
    "    model = tf.keras.models.Model(inputs=lstm_input, outputs=lstm_output)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=bs, verbose=0, callbacks=[early_stopping])\n",
    "       \n",
    "    # Testing\n",
    "    test = scaled_data[split-window:]\n",
    "    testX, testy = [], data[split:]\n",
    "    for i in range(window, len(test)):\n",
    "        testX.append(test[i-window:i, 0])\n",
    "    testX = np.array(testX)\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "    prediction = model.predict(testX)\n",
    "    prediction = (nmax-nmin)*prediction+nmin\n",
    "    prediction_series = pd.Series(prediction.flatten(), index=df.index[split:])\n",
    "        \n",
    "    # Forecast\n",
    "    results = pd.DataFrame()\n",
    "    for n in range(n_samples):\n",
    "        forecast_list = scaled_data[-window:]\n",
    "        lstm_input = tf.keras.layers.Input(shape=(trainX.shape[1], 1))\n",
    "        lstm_1 =  tf.keras.layers.LSTM(units=50, return_sequences=True)(lstm_input)\n",
    "        lstm_2 =  tf.keras.layers.LSTM(units=150)(lstm_1)\n",
    "        lstm_output = tf.keras.layers.Dense(units=1)(lstm_2)\n",
    "        model = tf.keras.models.Model(inputs=lstm_input, outputs=lstm_output)\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model.fit(trainX, trainy, epochs=epochs, batch_size=bs, verbose=0, callbacks=[early_stopping])\n",
    "        for _ in range(forecast):\n",
    "            x = forecast_list[-window:]\n",
    "            x = x.reshape((1, window, 1))\n",
    "            out = model.predict(x)[0][0]\n",
    "            forecast_list = np.append(forecast_list, out)\n",
    "\n",
    "        forecast_list = forecast_list[window-1:]\n",
    "        forecast_list = (nmax-nmin)*forecast_list+nmin\n",
    "        results['Forecast{}'.format(n+1)] = forecast_list.copy()\n",
    "        \n",
    "    last_date = df.index.values[-1]\n",
    "    forecast_dates = pd.date_range(last_date, periods=forecast+1).tolist()               \n",
    "    results['Date'] = forecast_dates \n",
    "    results.set_index('Date', inplace=True)\n",
    "    results['Mean'] = results.mean(axis=1)\n",
    "    results['_CI'] = results['Mean']-results.std(axis=1)/np.sqrt(n_samples)*1.96\n",
    "    results['+CI'] = results['Mean']+results.std(axis=1)/np.sqrt(n_samples)*1.96\n",
    "    \n",
    "    # Output\n",
    "    # print('Window: {} | Forecast: {} | Epochs: {} | Batch Size: {} | N_samples: {}'\\\n",
    "    #       .format(window, forecast, epochs, bs, n_samples))\n",
    "    # print('RMSE:', np.sqrt(np.mean(testy-prediction)**2))\n",
    "    \n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(df[cn][:split])\n",
    "        plt.plot(df[cn][split:])\n",
    "        plt.plot(prediction_series)\n",
    "        plt.plot(results['Mean'])\n",
    "        plt.fill_between(results.index, results['_CI'], results['+CI'], color='#ababab')\n",
    "        plt.legend(['Train', 'Test', 'Prediction', 'Forecast', 'CI'], ncol=5, loc='upper left')\n",
    "        plt.title('Window: {} | Forecast: {} | Epochs: {} | Batch Size: {} | N_samples: {} \\nRMSE: {}'\\\n",
    "                  .format(window, forecast, epochs, bs, n_samples, np.round(np.sqrt(np.mean(testy-prediction)**2)), 4), \n",
    "                  color='grey')\n",
    "        plt.suptitle('Projection: {} Forward Forecast'.format(cn), ha='left', x=.015, y=.95)\n",
    "        plt.figtext(0.85, 0.9, 'LSTM Model', ha='left') \n",
    "        fig.autofmt_xdate()\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Values')\n",
    "        ax.yaxis.set_label_position('right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_outbreak_model(df, cn, window, forecast, epochs, bs, seed=True, plot=True):\n",
    "    # Preprocessing                                                       \n",
    "    data = df.filter([cn]).values\n",
    "    nmin, nmax = data.min(), data.max()\n",
    "    scaled_data = ((df[cn]-nmin)/(nmax-nmin)).values.reshape(-1,1)\n",
    "    split = round(0.8*len(data)) \n",
    "    train = scaled_data[:split]\n",
    "    trainX, trainy = [], []\n",
    "    for i in range(window, len(train)):\n",
    "        trainX.append(train[i-window:i, 0])\n",
    "        trainy.append(train[i, 0])\n",
    "    trainX, trainy = np.array(trainX), np.array(trainy)\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "    \n",
    "    tf.random.set_seed(7) # set seed for reproducibility\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.003, patience=3, restore_best_weights=True)\n",
    "    \n",
    "    # Training\n",
    "    lstm_input = tf.keras.layers.Input(shape=(trainX.shape[1], 1))\n",
    "    lstm_1 = tf.keras.layers.LSTM(units=50, return_sequences=True)(lstm_input)\n",
    "    lstm_2 = tf.keras.layers.LSTM(units=150)(lstm_1)\n",
    "    lstm_output = tf.keras.layers.Dense(units=1)(lstm_2)\n",
    "    model = tf.keras.models.Model(inputs=lstm_input, outputs=lstm_output)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=[tf.keras.metrics.MeanSquaredError(name='mse'), tf.keras.metrics.MeanAbsoluteError(name='mae')])\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=bs, verbose=1, callbacks=[early_stopping])\n",
    "       \n",
    "    # Testing\n",
    "    test = scaled_data[split-window:]\n",
    "    testX, testy = [], data[split:]\n",
    "    for i in range(window, len(test)):\n",
    "        testX.append(test[i-window:i, 0])\n",
    "    testX = np.array(testX)\n",
    "    testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "    prediction = model.predict(testX)\n",
    "    prediction = (nmax-nmin)*prediction+nmin\n",
    "    prediction_series = pd.Series(prediction.flatten(), index=df.index[split:])\n",
    "        \n",
    "    # Forecast\n",
    "    forecast_list = scaled_data[-window:]\n",
    "    for _ in range(forecast):\n",
    "        x = forecast_list[-window:]\n",
    "        x = x.reshape((1, window, 1))\n",
    "        out = model.predict(x)[0][0]\n",
    "        forecast_list = np.append(forecast_list, out)\n",
    "\n",
    "    forecast_list = forecast_list[window-1:]\n",
    "    forecast_list = (nmax-nmin)*forecast_list+nmin\n",
    "        \n",
    "    last_date = df.index.values[-1]\n",
    "    forecast_dates = pd.date_range(last_date, periods=forecast+1).tolist()         \n",
    "\n",
    "    results = pd.DataFrame()\n",
    "    results['Forecast'] = forecast_list\n",
    "    results['Date'] = forecast_dates \n",
    "    results.set_index('Date', inplace=True)\n",
    "    \n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(10,7), dpi=100)\n",
    "        plt.plot(df[cn][:split])\n",
    "        plt.plot(df[cn][split:])\n",
    "        plt.plot(prediction_series)\n",
    "        plt.plot(results['Forecast'])\n",
    "        plt.legend(['Train', 'Test', 'Prediction', 'Forecast', 'CI'], ncol=5, loc='upper left')\n",
    "        plt.title('Window: {} | Forecast: {} | Epochs: {} | Batch Size: {} | RMSE: {}'\\\n",
    "                  .format(window, forecast, epochs, bs, np.round(np.sqrt(np.mean(testy-prediction)**2)), 4), \n",
    "                  color='grey')\n",
    "        plt.suptitle('Projection: {} Forward Forecast'.format(cn), ha='left', x=.015, y=1, fontsize=18)\n",
    "        plt.figtext(0.85, 0.9, 'LSTM Model', ha='left') \n",
    "        fig.autofmt_xdate()\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Cases')\n",
    "        ax.yaxis.set_label_position('right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 231714,
     "status": "ok",
     "timestamp": 1640181480255,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "8iA706VcYrYF",
    "outputId": "a556cf92-834d-4fde-88ba-c291abe897f7"
   },
   "outputs": [],
   "source": [
    "lstm_model(df=data, cn='cases', window=45, forecast=90, epochs=1, bs=8, plot=True)\n",
    "lstm_model(df=data, cn='cases', window=30, forecast=90, epochs=1, bs=8, plot=True)\n",
    "lstm_model(df=data, cn='cases', window=30, forecast=90, epochs=1, bs=16, plot=True)\n",
    "lstm_model(df=data, cn='cases', window=30, forecast=90, epochs=1, bs=16, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVzt6ewKYrYF"
   },
   "source": [
    "### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imPNlEQFYrYG"
   },
   "outputs": [],
   "source": [
    "def read_global():\n",
    "    all = pd.read_csv(current_directory + 'datasets/global_covid.csv', )\n",
    "    all['date'] = pd.to_datetime(all['date'], dayfirst=True)\n",
    "    all.set_index('date', inplace=True)\n",
    "    \n",
    "    return all\n",
    "\n",
    "all = read_global()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 2019,
     "status": "ok",
     "timestamp": 1640181564411,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "u1m0nmHFYrYG",
    "outputId": "a7fb8b54-9ddf-4e47-dfb1-512d083f75d8"
   },
   "outputs": [],
   "source": [
    "with plt.style.context('dark_background'): \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(all['total_cases'], label='Total Cases', linewidth=2)\n",
    "    plt.plot(all['cases'], label='Cases', linewidth=2)\n",
    "    plt.plot(all['total_deceased'], label='Total Deceased', linewidth=2)\n",
    "    plt.plot(all['deceased'], label='Deceased', linewidth=2)\n",
    "    ax.set_yscale('log')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Log Scale')\n",
    "    ax.yaxis.set_label_position('right')\n",
    "    plt.xlabel('Date')\n",
    "    plt.suptitle('Global Outlook', ha='left', x=.015, y=.95)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 214413,
     "status": "ok",
     "timestamp": 1640181781472,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "N1oFvfdlYrYG",
    "outputId": "4e023aba-15b1-4129-8abf-b82c8a240c87"
   },
   "outputs": [],
   "source": [
    "lstm_model(df=all, cn='cases', window=14, forecast=90, epochs=1, bs=8, n_samples=5, plot=True)\n",
    "lstm_model(df=all, cn='cases', window=14, forecast=90, epochs=1, bs=16, n_samples=5, plot=True)\n",
    "lstm_model(df=all, cn='cases', window=45, forecast=90, epochs=1, bs=8, n_samples=5, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 209073,
     "status": "ok",
     "timestamp": 1640181990533,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "PdNCgGf9YrYG",
    "outputId": "08bb8b5b-32a5-4e94-fd07-e99f6cb99c46"
   },
   "outputs": [],
   "source": [
    "lstm_model(df=all, cn='deceased', window=14, forecast=90, epochs=1, bs=8, n_samples=5, plot=True)\n",
    "lstm_model(df=all, cn='deceased', window=14, forecast=90, epochs=1, bs=16, n_samples=5, plot=True)\n",
    "lstm_model(df=all, cn='deceased', window=45, forecast=90, epochs=1, bs=8, n_samples=5, plot=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "8fac594bfae6525c0c41b4041d2d72effa188cc8ead05f81b1fab2bb098927fb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
