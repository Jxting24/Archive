{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b91099",
   "metadata": {
    "id": "f7ced672"
   },
   "source": [
    "# The Stock: General"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e1ee71",
   "metadata": {
    "id": "fa832c01"
   },
   "source": [
    "### Installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca8b96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8646,
     "status": "ok",
     "timestamp": 1640179506511,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "0c05ddd8",
    "outputId": "43591ca9-5a7a-4636-8e82-df7781be2b6a"
   },
   "outputs": [],
   "source": [
    "% pip install pandas==1.3.5 numpy==1.21.4 matplotlib==3.5.1 mplfinance==0.12.8b6 yfinance==0.1.63 scikit-learn==1.0.1 mlxtend==0.19.0 statsmodels==0.11.1 pmdarima==1.8.0 prophet==1.0.1 tensorflow==2.7.0 torch==1.9.0 bs4==0.0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7733509",
   "metadata": {
    "id": "R97m-NCjS2W0"
   },
   "source": [
    "### Connect Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa90c9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20287,
     "status": "ok",
     "timestamp": 1640179528197,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "ceuF5UpMS6Ph",
    "outputId": "c9177b5b-03cb-4bbb-82c8-0f8778a59415"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7557d5b7",
   "metadata": {
    "id": "SB5V1-W_iFgP"
   },
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4589b31",
   "metadata": {
    "id": "10C64uGEUfK1"
   },
   "outputs": [],
   "source": [
    "current_directory = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95dfbed",
   "metadata": {
    "id": "4caa87f8"
   },
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e0602",
   "metadata": {
    "id": "4f2b96eb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import mplfinance as mpf\n",
    "import yfinance as yf\n",
    "from sklearn import preprocessing, ensemble, model_selection\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import pmdarima as pm \n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "import tensorflow as tf \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from bs4 import BeautifulSoup\n",
    "import time, math, itertools, os, random, re, json, requests, copy\n",
    "import warnings; warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a734a3e",
   "metadata": {
    "id": "d92b9e2c"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8deed3",
   "metadata": {
    "id": "73f1f907"
   },
   "outputs": [],
   "source": [
    "# ticker_strings = ['MSFT', 'AAPL', 'GOOGL', 'FB', 'NVDA', 'TSLA', 'AMZN']\n",
    "\n",
    "# temp_df = list()\n",
    "# for ticker in ticker_strings:\n",
    "#     data = yf.download(ticker, group_by='Ticker', start='2000-07-07', end=\"2021-07-08\")\n",
    "#     data['Ticker'] = ticker  \n",
    "#     temp_df.append(data)\n",
    "\n",
    "# stock_df = pd.concat(temp_df)\n",
    "# stock_df.to_csv(current_directory + 'datasets/stock.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d03913",
   "metadata": {
    "id": "c97cbd06"
   },
   "source": [
    "### Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2c3b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 984
    },
    "executionInfo": {
     "elapsed": 2081,
     "status": "ok",
     "timestamp": 1640179698379,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "85d85afc",
    "outputId": "ff1420ef-7207-479a-b7f0-89b6f10bc301"
   },
   "outputs": [],
   "source": [
    "def ticker_financials(ticker:str):\n",
    "    input_ticker = copy.copy(ticker.upper())\n",
    "    url = 'https://finance.yahoo.com/quote/{}/financials?p={}'.format(input_ticker, input_ticker)\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36' } \n",
    "    response = requests.get(url, headers=headers, timeout=5)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    pattern = re.compile(r'\\s--\\sData\\s--\\s')\n",
    "    script_data = soup.find('script', text=pattern).contents[0]\n",
    "    start = script_data.find(\"context\")-2\n",
    "    json_data = json.loads(script_data[start:-12])\n",
    "    \n",
    "    annual_income_statement = json_data['context']['dispatcher']['stores']['QuoteSummaryStore']['incomeStatementHistory']['incomeStatementHistory']\n",
    "    quarterly_income_statement = json_data['context']['dispatcher']['stores']['QuoteSummaryStore']['incomeStatementHistoryQuarterly']['incomeStatementHistory']\n",
    "    annual_cashflow_statement = json_data['context']['dispatcher']['stores']['QuoteSummaryStore']['cashflowStatementHistory']['cashflowStatements']\n",
    "    quarterly_cashflow_statement = json_data['context']['dispatcher']['stores']['QuoteSummaryStore']['cashflowStatementHistoryQuarterly']['cashflowStatements']\n",
    "    annual_balance_sheet = json_data['context']['dispatcher']['stores']['QuoteSummaryStore']['balanceSheetHistory']['balanceSheetStatements']\n",
    "    quarterly_balance_sheet = json_data['context']['dispatcher']['stores']['QuoteSummaryStore']['balanceSheetHistoryQuarterly']['balanceSheetStatements']\n",
    "    \n",
    "    a_income_statement, q_income_statement, a_cashflow_statement, \\\n",
    "    q_cashflow_statement, a_balance_sheet, q_balance_sheet = ([] for i in range(6))\n",
    "\n",
    "    for s1, s2, s3, s4, s5, s6 in zip(annual_income_statement, \n",
    "                                      quarterly_income_statement, \n",
    "                                      annual_cashflow_statement, \n",
    "                                      quarterly_cashflow_statement, \n",
    "                                      annual_balance_sheet, \n",
    "                                      quarterly_balance_sheet):\n",
    "        \n",
    "        statement1, statement2, statement3, statement4, statement5, statement6 = ({} for i in range(6))\n",
    "        \n",
    "        for (k1, v1), (k2, v2), (k3, v3), (k4, v4), (k5, v5), (k6, v6) in zip(s1.items(), \n",
    "                                                                              s2.items(), \n",
    "                                                                              s3.items(), \n",
    "                                                                              s4.items(), \n",
    "                                                                              s5.items(), \n",
    "                                                                              s6.items()):\n",
    "            try:\n",
    "                statement1[k1] = v1['longFmt']\n",
    "                statement2[k2] = v2['longFmt']\n",
    "                statement3[k3] = v3['longFmt']\n",
    "                statement4[k4] = v4['longFmt']\n",
    "                statement5[k5] = v5['longFmt']\n",
    "                statement6[k6] = v6['longFmt']\n",
    "                \n",
    "            except TypeError:\n",
    "                continue\n",
    "                \n",
    "            except KeyError:\n",
    "                continue\n",
    "                \n",
    "        a_income_statement.append(statement1)\n",
    "        q_income_statement.append(statement2)\n",
    "        a_cashflow_statement.append(statement3)\n",
    "        q_cashflow_statement.append(statement4)\n",
    "        a_balance_sheet.append(statement5)\n",
    "        q_balance_sheet.append(statement6)\n",
    "        \n",
    "    a_income_statement = pd.DataFrame(a_income_statement)\n",
    "    q_income_statement = pd.DataFrame(q_income_statement)  \n",
    "    a_cashflow_statement = pd.DataFrame(a_cashflow_statement)\n",
    "    q_cashflow_statement = pd.DataFrame(q_cashflow_statement)\n",
    "    a_balance_sheet = pd.DataFrame(a_balance_sheet)\n",
    "    q_balance_sheet = pd.DataFrame(q_balance_sheet)\n",
    "    \n",
    "    return a_income_statement, q_income_statement, a_cashflow_statement, q_cashflow_statement, a_balance_sheet, q_balance_sheet\n",
    "\n",
    "a_is, q_is, a_cs, q_cs, a_bs, q_bs = ticker_financials(ticker='GOOGL')\n",
    "\n",
    "display(a_is) # Annual Income Statement\n",
    "display(q_is) # Quarter Income Statement\n",
    "display(a_cs) # Annual Cashflow Statement\n",
    "display(q_cs) # Quarter Cashflow Statement\n",
    "display(a_bs) # Annual Balancesheet Statement\n",
    "display(q_bs) # Quarter Balancesheet Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311d6a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3236,
     "status": "ok",
     "timestamp": 1640179703814,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "2089d92c",
    "outputId": "6203a542-ea8a-43f0-973f-2f2df55a5f10"
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def ticker_everything(ticker:str):\n",
    "    ticker = yf.Ticker(ticker)\n",
    "    ticker_info = pd.Series(ticker.info).to_frame().rename(columns={0: 'Value'}).rename_axis('Info')\n",
    "    ticker_actions = ticker.actions\n",
    "    ticker_afinancials = ticker.financials\n",
    "    ticker_qfinancials = ticker.quarterly_financials\n",
    "    ticker_abalancesheet = ticker.balancesheet\n",
    "    ticker_qbalancesheet = ticker.quarterly_balancesheet\n",
    "    ticker_cashflowstatement = ticker.cashflow\n",
    "    ticker_qcashflowstatement = ticker.quarterly_cashflow\n",
    "    ticker_earnings = ticker.earnings\n",
    "    ticker_sus = ticker.sustainability\n",
    "    ticker_recom = ticker.recommendations\n",
    "    ticker_cal = ticker.calendar\n",
    "    display(ticker_info,\n",
    "            ticker_actions,\n",
    "            ticker_afinancials,\n",
    "            ticker_qfinancials,\n",
    "            ticker_abalancesheet,\n",
    "            ticker_qbalancesheet,\n",
    "            ticker_cashflowstatement,\n",
    "            ticker_qcashflowstatement,\n",
    "            ticker_earnings,\n",
    "            ticker_sus,\n",
    "            ticker_recom,\n",
    "            ticker_cal)\n",
    "    \n",
    "ticker_everything(ticker='GOOGL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2a514",
   "metadata": {
    "id": "ce8906e7"
   },
   "source": [
    "### Statistic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5cbb35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1087,
     "status": "ok",
     "timestamp": 1640179722911,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "1df7180d",
    "outputId": "74d511aa-00b0-4078-d493-342417c7c510"
   },
   "outputs": [],
   "source": [
    "def ticker_statistics(ticker:str):\n",
    "    input_ticker = copy.copy(ticker.upper())\n",
    "    url = 'https://finance.yahoo.com/quote/{}/key-statistics?p={}'.format(input_ticker, input_ticker)\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36' } \n",
    "    response = requests.get(url, headers=headers, timeout=5)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    pattern = re.compile(r'\\s--\\sData\\s--\\s')\n",
    "    script_data = soup.find('script', text=pattern).contents[0]\n",
    "    start = script_data.find(\"context\")-2\n",
    "    json_data = json.loads(script_data[start:-12])\n",
    "    ticker_stats = json_data['context']['dispatcher']['stores']['QuoteSummaryStore']['defaultKeyStatistics']\n",
    "    ticker_stats = pd.DataFrame(ticker_stats).T\n",
    "    \n",
    "    return ticker_stats\n",
    "    \n",
    "ticker_stats = ticker_statistics(ticker='GOOGL')\n",
    "ticker_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac43d905",
   "metadata": {
    "id": "0f6e68dc"
   },
   "source": [
    "### Intrinsic Value \n",
    "\n",
    "To purchase a stock at the 'right place', intrinsic value or underlying value is one of the metrics to estimate does the current price of specific stock is underweight or overvalued based on the fundamental metrics of the company. Abundance formula has been formed to calculate the intrinsic value, here a simple formula to calculate the intrinsic value. The formula requires following value to compute the intrinsic value of the business which encompassing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746e1d72",
   "metadata": {
    "id": "adc0fb10"
   },
   "source": [
    "|      Params     |                  Name                 |   Unit   | Timeframe |         From        |\n",
    "|:---------------:|:-------------------------------------:|:--------:|:---------:|:-------------------:|\n",
    "|      T_OCF      |          Operating Cash Flow          | Millions |  Last 4Q  | Cash Flow Statement |\n",
    "|   T_Total_Debt  |               Total Debt              | Millions |   Last Q  |    Balance Sheet    |\n",
    "|      T_CSTI     |      Cash & Short Term Investment     | Millions |   Last Q  |    Balance Sheet    |\n",
    "|     T_CFGR1     |  Cash flow Growth rate next 1-5 years |  Percent |    ANY    |     EPS Estimate    |\n",
    "|     T_CFGR2     | Cash flow Growth rate next 6-10 years |  Percent |    ANY    |     EPS Estimate    |\n",
    "|      T_NSO      |         No. Shares Outstanding        |    No.   |   Recent  |          -          |\n",
    "|   T_Last_Close  |               Last Close              |   Price  | Yesterday |          -          |\n",
    "| T_Discount_Rate |             Discount Rate             |  Percent |    ANY    |         Beta        |\n",
    "|  T_next_n_year  |            Projected Years            |   Year   |    ANY    |         Self        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bcfb36",
   "metadata": {
    "id": "3f72744c"
   },
   "outputs": [],
   "source": [
    "def intrinsic_value(T_OCF, T_Total_Debt, T_CSTI, T_CFGR1, T_CFGR2, T_NSO, T_Last_Close, T_Discount_Rate, T_next_n_year):\n",
    "    # Initialization\n",
    "\n",
    "    OCF = T_OCF                                              \n",
    "    Total_Debt = T_Total_Debt                      \n",
    "    CSTI = T_CSTI  \n",
    "    CFGR1 = T_CFGR1/100                         \n",
    "    CFGR2 = T_CFGR2/100                         \n",
    "    NSO = T_NSO                           \n",
    "    Last_Close = T_Last_Close                     \n",
    "    Discount_Rate = T_Discount_Rate/100                 \n",
    "    \n",
    "    next_n_year = T_next_n_year                         \n",
    "    POCF = list()                            \n",
    "    POCF.append(OCF * ( 1 + CFGR1 ) ) \n",
    "    Discount_Factor = list()                                                  \n",
    "    Discount_Factor.append( 1 / ( 1 + Discount_Rate ) )\n",
    "    Discount_Value = list()                                                   \n",
    "    Discount_Value.append( POCF[-1] * Discount_Factor [-1] )\n",
    "\n",
    "    for i in range(next_n_year - 1):\n",
    "        POCF.append( POCF[-1] * ( 1 + CFGR1 ) )\n",
    "        Discount_Factor.append( Discount_Factor[-1] * ( 1 / ( 1 + Discount_Rate ) ) )\n",
    "        Discount_Value.append( POCF[-1] * Discount_Factor [-1] )\n",
    "        \n",
    "    total_n_year_CF = sum(Discount_Value)\n",
    "                          \n",
    "    INTRINSIC_VALUE_before_CashDebt = total_n_year_CF / NSO\n",
    "    Debt_per_share = Total_Debt / NSO\n",
    "    Cash_per_share = CSTI / NSO\n",
    "                          \n",
    "    INTRINSIC_VALUE = INTRINSIC_VALUE_before_CashDebt - Debt_per_share + Cash_per_share\n",
    "    Discount_OR_Premium = ( (Last_Close - INTRINSIC_VALUE) / INTRINSIC_VALUE ) * 100\n",
    "    \n",
    "    print('Current Instrinsic Value: {}'.format(round(INTRINSIC_VALUE, 4)))\n",
    "    print('Discount/Premium: {}'.format(round(Discount_OR_Premium, 4)))\n",
    "    \n",
    "#                                   NAME                                      UNIT       Timeframe    FROM\n",
    "intrinsic_value(T_OCF = ,           # Operating Cash Flow                     Millions   Last 4Q      Cash Flow Statement\n",
    "                T_Total_Debt = ,    # Total Debt                              Millions   Last Q       Balance Sheet\n",
    "                T_CSTI = ,          # Cash & Short Term Investment            Millions   Last Q       Balance Sheet\n",
    "                T_CFGR1 = ,         # Cash flow Growth rate next 1-5 years    Percent    ANY          EPS Estimate\n",
    "                T_CFGR2 = ,         # Cash flow Growth rate next 6-10 years   Percent    ANY          EPS Estimate\n",
    "                T_NSO = ,           # No. Shares Outstanding                  No.        Recent       -\n",
    "                T_Last_Close = ,    # Last Close                              Price      Yesterday    -\n",
    "                T_Discount_Rate = , # Discount Rate                           Percent    ANY          Beta\n",
    "                T_next_n_year = ,   # Projected Years                         Year       ANY          Self\n",
    "                )\n",
    "\n",
    "# T for temporary (ignore it)\n",
    "\n",
    "# Beta -> Discount Rate Conversion\n",
    "# <.8 = 4.6%\n",
    "# 1 ~ 1.5 = 5.6% ~ 8.1% | +0.5% per 0.1Beta\n",
    "# > 1.6 = 8.6% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac1bdd6",
   "metadata": {
    "id": "1aed9252"
   },
   "source": [
    "### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b6316b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "executionInfo": {
     "elapsed": 1716,
     "status": "ok",
     "timestamp": 1640179735142,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "7161fe03",
    "outputId": "7ef11bb2-18a6-4c15-f46c-d19e69e25873"
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    data = pd.read_csv(current_directory + 'datasets/stock.csv')\n",
    "    data['Date'] = pd.to_datetime(data['Date'], dayfirst=True)\n",
    "    data.set_index(['Date'], inplace=True)\n",
    "    display(data.head(1))\n",
    "    assert type(data.index) == pd.core.indexes.datetimes.DatetimeIndex\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = read_data()\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe949a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1640179737163,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "d7821b52",
    "outputId": "d1aed7ed-3595-4946-f2ee-88f2b4db2efd"
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56308a",
   "metadata": {
    "id": "b6468509"
   },
   "source": [
    "### Plotting Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e840aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1640179739465,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "a9f31a92",
    "outputId": "d9ab721e-428b-4159-fc2b-a66e3762a604"
   },
   "outputs": [],
   "source": [
    "custom_style = {\n",
    "    'figure.autolayout': True,      # Figure automatically adjust layout\n",
    "    'figure.titlesize': 20,         # Figure suptitle font size\n",
    "    'figure.figsize': (10, 5),      # Figure figsize\n",
    "    'figure.dpi': 100,              # Figure dots per inch\n",
    "    'axes.spines.top': False,       # Draw Axis spines top\n",
    "    'axes.spines.left': False,      # Draw Axis spin left  \n",
    "    'axes.titlesize': 10,           # Axes title font size\n",
    "    'axes.titlelocation': 'left',   # Axes title alignment\n",
    "    'axes.labelsize': 14,           # Axes label size\n",
    "    'axes.grid': True,              # Axes grid\n",
    "    'grid.color': '#969696',        # Axes grid col\n",
    "    'xtick.direction': 'inout',     # Xtick direction\n",
    "    'ytick.direction': 'inout',     # Ytick direction\n",
    "    'xtick.minor.visible': True,    # Draw Xtick minor \n",
    "    'ytick.minor.visible': True,    # Draw Ytick minor \n",
    "    'ytick.right': True,            # Draw ticks right\n",
    "    'ytick.left': False,            # Draw ticks left\n",
    "    'ytick.labelright': True,       # Draw ticks label right\n",
    "    'ytick.labelleft': False,       # Draw ticks label left\n",
    "    'xaxis.labellocation': 'right', # Xaxis alignment\n",
    "    'yaxis.labellocation': 'top',   # Yaxis alignment\n",
    "    'font.family': 'serif',    # Figure font \n",
    "    'legend.fontsize': 10,          # Legend font size\n",
    "    'legend.loc': 'best',           # Legend location\n",
    "}\n",
    "\n",
    "print('========================================')\n",
    "print('Auto Configured:')\n",
    "print('========================================')\n",
    "for k, v in zip(list(custom_style.keys()), list(custom_style.values())):\n",
    "    n = 30 - len(k)\n",
    "    print(str(k) + str(' '*n) + str(v))\n",
    "print('========================================')\n",
    "print('Further Configure: ')\n",
    "print('========================================')\n",
    "print('ax.yaxis.set_label_position(\"right\")')\n",
    "print('mpl.rcParams.update(mpl.rcParamsDefault)')\n",
    "  \n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format='svg'\n",
    "\n",
    "plt.style.use(custom_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7dc87",
   "metadata": {
    "id": "332f0a25"
   },
   "source": [
    "### History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9353d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "executionInfo": {
     "elapsed": 1832,
     "status": "ok",
     "timestamp": 1640179744847,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "118ce797",
    "outputId": "5ad839f2-367a-4f07-d8eb-118758da86f1"
   },
   "outputs": [],
   "source": [
    "def history_performance(df):\n",
    "    scaled_df = list()\n",
    "    floating = df.select_dtypes(include=['float64']).columns.values # Select columns with floating data type\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    for t in df['Ticker'].unique():\n",
    "        temp = df[df['Ticker'] == t].copy()\n",
    "        temp[floating] = scaler.fit_transform(temp[floating])\n",
    "        scaled_df.append(temp)\n",
    "        \n",
    "    scaled_df = pd.concat(scaled_df)\n",
    "    \n",
    "    color_list = ['#00a4ef', '#a2aaad', '#fbbc05', '#4267b2', '#76b900', '#e82127', '#ff9900', '#00a270']\n",
    "    fig, ax = plt.subplots()\n",
    "    for t, c in zip(scaled_df['Ticker'].unique(), color_list):\n",
    "        plt.plot(scaled_df[scaled_df['Ticker'] == t].index, scaled_df[scaled_df['Ticker'] == t]['Close'], c=c, label=t)\n",
    "    plt.suptitle('History', ha='left', x=.015, y=1); plt.title('Established')\n",
    "    plt.legend(); plt.ylabel('Scaled Values'); plt.xlabel('Date'); ax.yaxis.set_label_position(\"right\")\n",
    "    plt.show()\n",
    "    \n",
    "history_performance(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179952b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 8938,
     "status": "ok",
     "timestamp": 1640179756707,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "cf2b2615",
    "outputId": "77f7430f-3992-4667-a577-9de135f02528"
   },
   "outputs": [],
   "source": [
    "def plot_ohlc(df, ticker):\n",
    "    df = df[df['Ticker'] == ticker].tail(30).reset_index()\n",
    "    x = np.arange(0,len(df))\n",
    "    fig, (ax, ax2) = plt.subplots(2, figsize=(10,5), gridspec_kw={'height_ratios': [4, 1]}, dpi=100)\n",
    "    for idx, value in df.iterrows():\n",
    "        color = '#2CA453'\n",
    "        if value['Open'] > value['Close']: color= '#F04730'\n",
    "        ax.plot([x[idx], x[idx]], [value['Low'], value['High']], color=color)\n",
    "        ax.plot([x[idx], x[idx]-0.1], [value['Open'], value['Open']], color=color)\n",
    "        ax.plot([x[idx], x[idx]+0.1], [value['Close'], value['Close']], color=color)\n",
    "               \n",
    "    ax2.bar(x, df['Volume'], color='lightgrey')\n",
    "    max_ = df['Volume'].max()*1.1\n",
    "    yticks_ax2 = np.arange(0, max_+1, max_/4)\n",
    "    yticks_labels_ax2 = ['{:.2f} M'.format(i/1000000) for i in yticks_ax2]\n",
    "    plt.yticks(yticks_ax2[1:-1], yticks_labels_ax2[1:-1]); plt.ylim(0,max_)\n",
    "    \n",
    "    ax.set_xticks(x, minor=True); ax.set_ylabel('Price');\n",
    "    ax2.set_xticks(x[::3]); ax2.set_xticklabels(df.Date.dt.date[::3]); ax2.set_ylabel('Volume'); ax2.yaxis.tick_right() \n",
    "    \n",
    "    ax.grid(axis='y'); ax2.grid(False)\n",
    "    \n",
    "    plt.suptitle('Candlestick: {}'.format(ticker), ha='left', x=.015, y=1)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    fig.autofmt_xdate()\n",
    "               \n",
    "def plot_candlestick(df, ticker):\n",
    "    df = df[df['Ticker'] == t].tail(180)\n",
    "    mpf.plot(df, figratio=(10,5), type='candle', mav=(7, 21), volume=True, title=ticker, style='classic')\n",
    "    \n",
    "for t in data['Ticker'].unique():\n",
    "    # plot_ohlc(data, ticker=t)\n",
    "    plot_candlestick(data, ticker=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb195ff",
   "metadata": {
    "id": "489160b0"
   },
   "outputs": [],
   "source": [
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.style.use(custom_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da096c0",
   "metadata": {
    "id": "0463a0f8"
   },
   "source": [
    "### Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588fafc",
   "metadata": {
    "id": "23a520b8"
   },
   "outputs": [],
   "source": [
    "def some_indicators(df, o, c, h, l, v):\n",
    "    result = list()\n",
    "    for t in df['Ticker'].unique():\n",
    "        temp = df[df['Ticker'] == t].copy()\n",
    "    \n",
    "    # Trend\n",
    "    \n",
    "        # Simple Moving Average\n",
    "        temp['SMA20'] = temp[c].rolling(window = 20).mean()\n",
    "        temp['SMA50'] = temp[c].rolling(window = 50).mean()\n",
    "        temp['SMA150'] = temp[c].rolling(window = 150).mean()\n",
    "        temp['SMA200'] = temp[c].rolling(window = 200).mean()\n",
    "        \n",
    "        # Exponential Moving Average\n",
    "        temp['EMA20'] = temp[c].ewm(span = 20).mean()\n",
    "        temp['EMA40'] = temp[c].ewm(span = 40).mean()\n",
    "        \n",
    "        # Weighted Moving Average\n",
    "        weights = np.array([0.4, 0.2, 0.2, 0.1, 0.1])\n",
    "        sum_weights = np.sum(weights)\n",
    "        temp['WMA5'] = (temp[c].rolling(window = 5).apply(lambda x: np.sum(weights * x) / sum_weights, raw = False))\n",
    "        \n",
    "        # Moving Average Convergence Divergence \n",
    "        temp['EMA26'] = temp[[c]].ewm(span = 26).mean()\n",
    "        temp['EMA12'] = temp[[c]].ewm(span = 12).mean()\n",
    "        temp['MACD'] = temp['EMA12'] - temp['EMA26']\n",
    "        \n",
    "    # Volatility\n",
    "    \n",
    "        # Bollinger Band\n",
    "        temp['20SD'] = temp[c].rolling(window=20).std() \n",
    "        temp['UB'] = temp['SMA20'] + (temp['20SD'] * 2)\n",
    "        temp['LB'] = temp['SMA20'] - (temp['20SD'] * 2)\n",
    "        \n",
    "        # Average True Range\n",
    "        atr_window = 14\n",
    "        Previous_close = temp[c].shift(1)\n",
    "        true_range = pd.DataFrame(data={'tr1': temp[h] - temp[l],\n",
    "                                        'tr2': (temp[h] - Previous_close).abs(),\n",
    "                                        'tr3': (temp[l] - Previous_close).abs()}).max(axis=1)\n",
    "        ATR = np.zeros(len(temp[c]))\n",
    "        ATR[atr_window - 1] = true_range[0: atr_window].mean()\n",
    "        for i in range(atr_window, len(ATR)):\n",
    "            ATR[i] = (ATR[i - 1] * (atr_window - 1) + true_range.iloc[i]) / float(atr_window)\n",
    "        ATR = pd.Series(data = ATR, index=true_range.index)\n",
    "        temp.insert(temp.shape[1], 'ATR', ATR)\n",
    "        \n",
    "    # Momentum\n",
    "    \n",
    "        # Stochastic Oscillator \n",
    "        stoch_window = 14; stoch_smooth_window = 3; stoch_periods = 14\n",
    "        S_min = temp[l].rolling(stoch_window, min_periods = stoch_periods).min()\n",
    "        S_max = temp[h].rolling(stoch_window, min_periods = stoch_periods).max()\n",
    "        STOCH = 100 * (temp[c] - S_min) / (S_max - S_min)\n",
    "        temp.insert(temp.shape[1], 'STOCH', STOCH)\n",
    "        \n",
    "        # Relative Strength Index\n",
    "        rsi_periods = 14; diff = temp[c].diff(1)\n",
    "        UP_direction = diff.where(diff > 0, 0.0); \n",
    "        DOWN_direction = -diff.where(diff < 0, 0.0)\n",
    "        EMA_up = UP_direction.ewm(alpha = 1 / rsi_periods, min_periods = rsi_periods, adjust = rsi_periods).mean()\n",
    "        EMA_down = DOWN_direction.ewm(alpha = 1 / rsi_periods, min_periods = rsi_periods, adjust = rsi_periods).mean()\n",
    "        RSI = pd.Series(np.where(EMA_down == 0, 100, 100 - (100 / (1 + (EMA_up / EMA_down)))), index = temp[c].index)\n",
    "        temp.insert(temp.shape[1], 'RSI', RSI)\n",
    "        \n",
    "    # Volume\n",
    "    \n",
    "        # Accumulation/Distribution Index\n",
    "        CLV = (((temp[c] - temp[l]) - (temp[h] - temp[c]))/(temp[h] - temp[l])).fillna(0.0)\n",
    "        ADI = CLV * temp[v]\n",
    "        temp['ADI'] = ADI.cumsum()\n",
    "        \n",
    "        # On-Balance Volume\n",
    "        OBV = np.where(temp[c] < temp[c].shift(1), -temp[v], temp[v])\n",
    "        OBV = pd.Series(OBV, index = temp[c].index).cumsum()\n",
    "        temp.insert(temp.shape[1], 'OBV', OBV)\n",
    "           \n",
    "            \n",
    "        result.append(temp)\n",
    "\n",
    "    result = pd.concat(result)\n",
    "    return result\n",
    "\n",
    "data = some_indicators(df=data, o='Open', c='Close', h='High', l='Low', v='Volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e19f32",
   "metadata": {
    "id": "de7193dc"
   },
   "outputs": [],
   "source": [
    "def technical_chart(ticker_name, ticker_df, last_n, plot=True):\n",
    "    assert type(ticker_name) == str\n",
    "    \n",
    "    if plot:\n",
    "      ticker_df = ticker_df.tail(last_n)\n",
    "      fig, ax = plt.subplots(figsize=(10,5), dpi=150)\n",
    "      ax.plot(ticker_df.index, ticker_df['Close'], c='black', label='Close')\n",
    "      ax.plot(ticker_df.index, ticker_df['SMA50'], c='b', label='SMA50')\n",
    "      ax.plot(ticker_df.index, ticker_df['SMA150'], c='g', label='SMA150')\n",
    "      ax.plot(ticker_df.index, ticker_df['SMA200'], c='r', label='SMA200')\n",
    "      ax.fill_between(ticker_df.index, ticker_df['LB'], ticker_df['UB'], alpha=0.35, label='Bollinger Band')\n",
    "      ax.legend(loc='lower right')\n",
    "\n",
    "      zoom_df = ticker_df.tail(last_n-math.ceil(last_n*0.9))\n",
    "      ax2 = fig.add_axes([0.01, 0.55, 0.25, 0.30]) # left, bottom, width, height\n",
    "      ax2.plot(zoom_df.index, zoom_df['Close'], label='Close')\n",
    "      ax2.plot(zoom_df.index, zoom_df['EMA20'], label='EMA20')\n",
    "      ax2.plot(zoom_df.index, zoom_df['EMA40'], label='EMA40')\n",
    "      ax2.legend(prop={'size': 5})\n",
    "      ax2.xaxis.set_major_locator(mdates.MonthLocator(interval=12))\n",
    "      \n",
    "      plt.suptitle('{}'.format(ticker_name), ha='left', x=.015, y=1); plt.title('Short Term Performance')\n",
    "\n",
    "for t in data['Ticker'].unique():\n",
    "    technical_chart(t, data[data['Ticker'] == t], last_n=999, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e683ff",
   "metadata": {
    "id": "693ab6e6"
   },
   "outputs": [],
   "source": [
    "def technical_chart2(ticker_name, ticker_df, last_n, plot=True):\n",
    "    assert type(ticker_name) == str\n",
    "    \n",
    "    if plot:\n",
    "      ticker_df = ticker_df.tail(last_n)\n",
    "      fig, axs = plt.subplots(3, 2, figsize=(10,7), dpi=100)\n",
    "      axs[0,0].plot(ticker_df.index, ticker_df['MACD']); axs[0,0].set_title('Moving Average Convergence Divergence')\n",
    "      axs[0,1].plot(ticker_df.index, ticker_df['ATR']); axs[0,1].set_title('Average True Range')\n",
    "      axs[1,0].plot(ticker_df.index, ticker_df['STOCH']); axs[1,0].set_title('Stochastic Oscillator')\n",
    "      axs[1,1].plot(ticker_df.index, ticker_df['RSI']); axs[1,1].set_title('Relative Strength Index')\n",
    "      axs[2,0].plot(ticker_df.index, ticker_df['ADI']); axs[2,0].set_title('Accumulation/Distribution Index')\n",
    "      axs[2,1].plot(ticker_df.index, ticker_df['OBV']); axs[2,1].set_title('On-Balance Volume')\n",
    "      plt.suptitle('Technical Indicator: {}'.format(ticker_name), ha='left', x=.015, y=1)\n",
    "\n",
    "for t in data['Ticker'].unique():\n",
    "    technical_chart2(t, data[data['Ticker'] == str(t)], last_n=100, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c3091",
   "metadata": {
    "id": "0debaca5"
   },
   "source": [
    "### Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b1b78",
   "metadata": {
    "id": "26dd041e"
   },
   "outputs": [],
   "source": [
    "def ticker_separation(data):\n",
    "    msft = data[data['Ticker'] == 'MSFT'].drop(columns='Ticker')\n",
    "    aapl = data[data['Ticker'] == 'AAPL'].drop(columns='Ticker')\n",
    "    googl = data[data['Ticker'] == 'GOOGL'].drop(columns='Ticker')\n",
    "    fb = data[data['Ticker'] == 'FB'].drop(columns='Ticker')\n",
    "    nvda = data[data['Ticker'] == 'NVDA'].drop(columns='Ticker')\n",
    "    tsla = data[data['Ticker'] == 'TSLA'].drop(columns='Ticker')\n",
    "    amzn = data[data['Ticker'] == 'AMZN'].drop(columns='Ticker')\n",
    "\n",
    "    ticker_dict = {'MSFT': msft, 'AAPL': aapl, 'GOOGL': googl, 'FB': fb, 'NVDA': nvda, 'TSLA': tsla, 'AMZN': amzn}\n",
    "    \n",
    "    return ticker_dict\n",
    "\n",
    "ticker_dict = ticker_separation(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0490969",
   "metadata": {
    "id": "6a62a41a"
   },
   "source": [
    "### Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afb9d9",
   "metadata": {
    "id": "5a0eb444"
   },
   "outputs": [],
   "source": [
    "def fourier_transformation(df, tn, plot=True):\n",
    "    assert type(tn) == str\n",
    "    \n",
    "    fft_df = pd.DataFrame({'fft': np.fft.fft(np.asarray(df['Close']))})\n",
    "    fft_df['Absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))\n",
    "    fft_df['Angle'] = fft_df['fft'].apply(lambda x: np.angle(x))\n",
    "    \n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "        fftlist = np.asarray(fft_df['fft'])\n",
    "        for n in [3, 6, 9, 78]:\n",
    "            fftlistc = np.copy(fftlist); fftlistc[n:-n] = 0\n",
    "            ax.plot(np.fft.ifft(fftlistc), label='FT Component {}'.format(n))\n",
    "        ax.plot(df['Close'],  label='Close'); \n",
    "        ax.yaxis.set_label_position(\"right\"); \n",
    "        plt.legend(); plt.xlabel('Index'); plt.ylabel('Close'); \n",
    "        plt.suptitle('Fourier Tranform: {}'.format(tn), ha='left', x=.015, y=1)\n",
    "        plt.show()\n",
    "    \n",
    "for tn,tdf in zip(list(ticker_dict.keys()), list(ticker_dict.values())):\n",
    "    fourier_transformation(df=tdf.reset_index()[['Date', 'Close']], tn=tn, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05165205",
   "metadata": {
    "id": "29ce03a9"
   },
   "source": [
    "### Hyperparameter Tuning for ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92a297",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 403424,
     "status": "ok",
     "timestamp": 1640180258316,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "3fb13ec8",
    "outputId": "901974ff-f8e6-49ae-efdc-fcf5920a3677"
   },
   "outputs": [],
   "source": [
    "def optimal_arima(df, tn, cn):\n",
    "    assert type(tn) == str\n",
    "    \n",
    "    print('Ticker: {}'.format(tn))\n",
    "    oarima_model = pm.auto_arima(df[cn], start_p=1, start_q=1, test='adf', max_p=5, max_q=5, max_d=5,\n",
    "                                 seasonal=False, trace=True, random_state=7)\n",
    "    # print('')\n",
    "    # display(oarima_model.summary())\n",
    "    # return oarima_model.order\n",
    "    \n",
    "arima_params = dict()\n",
    "for tn,tdf in zip(list(ticker_dict.keys()), list(ticker_dict.values())):\n",
    "    optimal_arima(df=tdf, tn=tn, cn='Close')\n",
    "    # arima_params[tn] = optimal_arima(df=tdf, tn=tn, cn='Close')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfdd6f6",
   "metadata": {
    "id": "6fedde66"
   },
   "source": [
    "### ARIMA Model Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9bc997",
   "metadata": {
    "id": "bf69e5eb"
   },
   "outputs": [],
   "source": [
    "def arima_model(df, tn, cn, params, n_days, forecast_days, plot=True):\n",
    "    assert type(tn) == str\n",
    "    assert type(cn) == str\n",
    "    \n",
    "    if n_days > len(df):\n",
    "        raise Exception('n_days > length of data')\n",
    "    else:\n",
    "        warnings.filterwarnings('ignore')\n",
    "        start = time.time()\n",
    "\n",
    "        recent = df.tail(n_days).copy()\n",
    "        series = recent[cn]\n",
    "        arima = ARIMA(series, order=params)\n",
    "        arima_fitted = arima.fit(disp=0)\n",
    "        # print(arima_fitted.summary())\n",
    "        # autocorrelation_plot(series)\n",
    "        X = series.values; split = int(len(X)*0.70)\n",
    "        train, test = X[0:split], X[split:len(X)]\n",
    "        history = [x for x in train]\n",
    "        predictions = list()\n",
    "        for t in range(len(test)):\n",
    "            arima = ARIMA(history, order=(5,1,0))\n",
    "            arima_fitted = arima.fit(disp=0)\n",
    "            predictions.append(arima_fitted.forecast()[0])\n",
    "            obs = test[t]\n",
    "            history.append(obs)\n",
    "\n",
    "        error = np.round(np.mean((test-predictions)**2),4);\n",
    "    \n",
    "        forecast, se, ci = arima_fitted.forecast(forecast_days, alpha=0.05)\n",
    "        forecast_date = pd.date_range(recent.index[-1], periods=forecast_days).tolist()\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "            ax.plot(recent.index[:len(train)], train, label='Training')\n",
    "            ax.plot(recent.index[-len(test):], test, label='Testing')\n",
    "            ax.plot(recent.index[-len(test):], predictions, color='r', label='Predicted')\n",
    "            ax.plot(forecast_date, forecast, c='black', label='Forecast')\n",
    "            ax.fill_between(forecast_date, ci[:, 0], ci[:, 1], alpha=.35)\n",
    "            ax.yaxis.set_label_position(\"right\"); \n",
    "            plt.legend(loc='upper left'); plt.xlabel('Days'); plt.ylabel(cn); \n",
    "            plt.suptitle('ARIMA Model: {}'.format(tn), ha='left', x=.015, y=1)\n",
    "            plt.title('MSE: {} | Run Time: {} | Params(p,q,d): {} | Forecast days: {}'\\\n",
    "                      .format(error, round(end-start,6), params, forecast_days), c='grey')\n",
    "            plt.show()\n",
    "\n",
    "for tn,tdf,params in zip(list(ticker_dict.keys()), list(ticker_dict.values()), list(arima_params.values())):\n",
    "    arima_model(df=tdf, tn=tn, cn='Close', params=params, n_days=200, forecast_days=30, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7766fa",
   "metadata": {
    "id": "b99087d6"
   },
   "source": [
    "### Prophet Model Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ec739",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63198,
     "status": "ok",
     "timestamp": 1640180363090,
     "user": {
      "displayName": "Wei Jie Lee Kiong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11519924154603132072"
     },
     "user_tz": -480
    },
    "id": "47d89e63",
    "outputId": "6c07f7b3-50ea-46ee-8c63-8ea8fdd987e8"
   },
   "outputs": [],
   "source": [
    "def prophet_model(df, tn, cn, n_days, forecast_days, plot=True):\n",
    "    assert type(tn) == str\n",
    "    assert type(cn) == str\n",
    "    \n",
    "    short = df.tail(n_days).copy()\n",
    "    \n",
    "    # param_grid = {  \n",
    "    #     'changepoint_prior_scale': [0.001, 0.01, 0.1],\n",
    "    #     'seasonality_prior_scale': [0.01, 0.1, 1.0]}\n",
    "    # all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "    # errors = list()\n",
    "    # cutoffs = pd.to_datetime(['2018-07-08', '2019-01-01', '2019-07-08'])\n",
    "\n",
    "    # for params in all_params:\n",
    "    #      fbp = Prophet(**params, daily_seasonality=True)\\\n",
    "    #     .fit(short.reset_index().rename(columns={'Date': 'ds', cn: 'y'})[['ds', 'y']])\n",
    "    #     cv = cross_validation(fbp, cutoffs=cutoffs, horizon='365 days')\n",
    "    #     pm = performance_metrics(cv, rolling_window=1)\n",
    "    #     errors.append(pm['rmse'].values[0])\n",
    "        \n",
    "    # tuning_results = pd.DataFrame(all_params)\n",
    "    # tuning_results['rmse'] = errors\n",
    "    # display(tuning_results)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    fbp = Prophet(changepoint_prior_scale=0.001, seasonality_prior_scale=0.01, daily_seasonality=True)\n",
    "    fbp.fit(short.reset_index().rename(columns={'Date': 'ds', cn: 'y'})[['ds', 'y']])\n",
    "    future = fbp.make_future_dataframe(periods=forecast_days)\n",
    "    forecast = fbp.predict(future)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(forecast['ds'][:-forecast_days], forecast['yhat'][:-forecast_days], label='Training', c='black')\n",
    "        ax.plot(forecast['ds'][-forecast_days:], forecast['yhat'][-forecast_days:], label='Forecast', c='#0072b2')\n",
    "        ax.fill_between(forecast['ds'][-forecast_days:], \n",
    "                        forecast['yhat_lower'][-forecast_days:], \n",
    "                        forecast['yhat_upper'][-forecast_days:], label='CI', color='#bedaea')\n",
    "        ax.yaxis.set_label_position(\"right\"); plt.xlabel('Date'); plt.ylabel(cn); plt.legend(); \n",
    "        plt.suptitle('Prophet: {}'.format(tn), ha='left', x=.015, y=1); \n",
    "        plt.title('Commence: {} | End: {} | Run Time: {} | Forecast days: {}'\\\n",
    "                  .format(str(short.index.min())[:10], str(short.index.max())[:10], round(end-start,6), forecast_days), \n",
    "                  c='grey')\n",
    "        plt.show()\n",
    "\n",
    "for tn,tdf in zip(list(ticker_dict.keys()), list(ticker_dict.values())):\n",
    "    prophet_model(df=tdf, tn=tn, cn='Close', n_days=1000, forecast_days=300, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30ed80",
   "metadata": {
    "id": "f100be24"
   },
   "source": [
    "### Feature Selection for Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c895c8d",
   "metadata": {
    "id": "c0d45087"
   },
   "outputs": [],
   "source": [
    "def feature_selection(df, tn, cn, ntf, plot=True):\n",
    "    assert type(tn) == str\n",
    "    assert type(cn) == str\n",
    "    \n",
    "    X = df.drop(columns=[cn]).fillna(0)\n",
    "    y = df[cn].values\n",
    "\n",
    "    etc = ensemble.ExtraTreesRegressor(random_state=7)\n",
    "    etc.fit(X, y)\n",
    "    feature_table = pd.DataFrame({'Count': [f for f in range(len(etc.feature_importances_))], \n",
    "                                  'FI': etc.feature_importances_.tolist(), 'Label': [f for f in df.columns if f !=str(cn)]})\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "        ax.bar(feature_table['Count'], feature_table['FI'], tick_label=feature_table['Label'])\n",
    "        plt.suptitle('Feature Importances: {}'.format(tn), ha='left', x=.015, y=1)\n",
    "        plt.title('With Extra Tree Regressor', c='grey')\n",
    "    \n",
    "    return feature_table.sort_values(by=['FI'], ascending=False)['Label'].tolist()[:ntf]\n",
    "    \n",
    "selected_feature_dict = {}\n",
    "for tn,tdf in zip(list(ticker_dict.keys()), list(ticker_dict.values())):\n",
    "    selected_feature = feature_selection(df=tdf, tn=tn, cn='Close', ntf=8, plot=False)\n",
    "    selected_feature.append('Close')\n",
    "    selected_feature_dict[tn] = selected_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74fe655",
   "metadata": {
    "id": "d3b44a7e"
   },
   "source": [
    "### Autoencoder Dimentionality Reduction and Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5645fd7a",
   "metadata": {
    "id": "817fd796"
   },
   "outputs": [],
   "source": [
    "def autoencoder_model(df, tn, cn, plot=True):\n",
    "    assert type(tn) == str\n",
    "    assert type(cn) == str\n",
    "    \n",
    "    trainX = df.sample(frac=.8, random_state=7); testX = df.drop(trainX.index)   \n",
    "    \n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    trainX_scaled = scaler.fit_transform(trainX.copy())\n",
    "    testX_scaled = scaler.transform(testX.copy())\n",
    "\n",
    "    tf.random.set_seed(7) # set seed for reproducibility\n",
    "    \n",
    "    class AutoEncoder(tf.keras.Model):\n",
    "        def __init__(self, output_units, code_size=8):\n",
    "            super().__init__()\n",
    "            self.encoder = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.1),\n",
    "                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.1),\n",
    "                tf.keras.layers.Dense(16, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.1),\n",
    "                tf.keras.layers.Dense(code_size, activation='relu')\n",
    "            ])\n",
    "            self.decoder = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(16, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.1),\n",
    "                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.1),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.1),\n",
    "                tf.keras.layers.Dense(output_units, activation='sigmoid')\n",
    "            ])\n",
    "        def call(self, inputs):\n",
    "            encoded = self.encoder(inputs)\n",
    "            decoded = self.decoder(encoded)\n",
    "            return decoded\n",
    "        \n",
    "    \n",
    "    model = AutoEncoder(output_units=trainX_scaled.shape[1])\n",
    "    model.compile(loss='msle', metrics=['mse'], optimizer='adam') # Mean Squared Logarithmic loss robust to outliers\n",
    "    history = model.fit(trainX_scaled, trainX_scaled, verbose=0, epochs=20, batch_size=512, \n",
    "                        validation_data=(testX_scaled, testX_scaled))\n",
    "\n",
    "    if plot:\n",
    "      fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "      ax.plot(history.history['loss'], label='Training Loss')\n",
    "      ax.plot(history.history['val_loss'], label='Validation Loss')\n",
    "      ax.yaxis.set_label_position(\"right\")\n",
    "      plt.legend() \n",
    "      plt.xlabel('Epochs')\n",
    "      plt.ylabel('MSLE Loss') \n",
    "      plt.title('History Loss')\n",
    "      plt.suptitle('AutoEncoder Model: {}'.format(tn), ha='left', x=.015, y=1)\n",
    "\n",
    "    whole = np.concatenate((trainX_scaled, testX_scaled), axis=0) # Concat train & test\n",
    "    reconstructed = model.predict(whole)\n",
    "    reconstructed_errors = tf.keras.losses.msle(whole, reconstructed)\n",
    "    outliers_threshold = np.mean(reconstructed_errors.numpy()) + np.std(reconstructed_errors.numpy())\n",
    "    anomaly_mask = pd.Series(reconstructed_errors) > outliers_threshold\n",
    "    reconstructed_df = pd.DataFrame(reconstructed, columns=df.columns)\n",
    "    reconstructed_df['Outliers'] = anomaly_mask.map(lambda x: 'Anomaly' if x == True else 'Normal')\n",
    "    \n",
    "    if plot:\n",
    "      print(reconstructed_df['Outliers'].value_counts())\n",
    "    \n",
    "    if plot:\n",
    "      fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "      bar = reconstructed_df['Outliers'].value_counts().plot(kind='bar', title='Outliers Counts', ylabel='Counts')\n",
    "      ax.yaxis.set_label_position(\"right\");\n",
    "      for b in bar.patches:\n",
    "          bar.annotate(format(b.get_height(),'.2f'), (b.get_x()+b.get_width()/2, b.get_height()), \n",
    "                      ha='center', va='center', size=10, xytext=(0, 8), textcoords='offset points')\n",
    "    \n",
    "for tn,tdf in zip(list(ticker_dict.keys()), list(ticker_dict.values())):\n",
    "    autoencoder_model(df=tdf.reset_index(drop=True)[selected_feature].fillna(0), tn=tn, cn='Close', plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377eb5c3",
   "metadata": {
    "id": "0cdec542"
   },
   "source": [
    "### Time Series Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85ed01b",
   "metadata": {
    "id": "7f6925e2"
   },
   "outputs": [],
   "source": [
    "filtered = data.loc['2017-01-01':]\n",
    "filtered_ticker_dict = dict(tuple(filtered.groupby('Ticker')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229eb2bb",
   "metadata": {
    "id": "101b7ecb"
   },
   "outputs": [],
   "source": [
    "r = pd.DataFrame()\n",
    "for tn,tdf in zip(list(filtered_ticker_dict.keys()), list(filtered_ticker_dict.values())):\n",
    "    r = r.add(tdf.iloc[:, 0:6], fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730b638",
   "metadata": {
    "id": "c4c2a7b2"
   },
   "outputs": [],
   "source": [
    "ngpu = torch.cuda.device_count()\n",
    "for i in range(ngpu):\n",
    "    print(\"GPU {}: {}\".format(i+1, torch.cuda.get_device_name(i)))\n",
    "    \n",
    "device = torch.device('cuda:0' if (torch.cuda.is_available() and ngpu > 0) else 'cpu')\n",
    "num_epochs = 100000\n",
    "evaluation_epoch_num = 10000\n",
    "batch_size = 64\n",
    "optimizer_betas = (0.9, 0.999)\n",
    "learning_rate = 5.125e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ea743",
   "metadata": {
    "id": "131f4ac9"
   },
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):\n",
    "    def __init__(self, data_frame, sequence_length=2):\n",
    "        self.data = torch.tensor(data_frame.values)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index: index + self.sequence_length].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff39fce",
   "metadata": {
    "id": "41eb0808"
   },
   "outputs": [],
   "source": [
    "training_columns_list = ['Close', 'Open', 'High', 'Low']\n",
    "data_dimension = len(training_columns_list)\n",
    "sequence_length = 7\n",
    "\n",
    "train_data, evaluation_data = model_selection.train_test_split(r[training_columns_list], \n",
    "                                                               test_size=0.2, shuffle=False)\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(train_data)\n",
    "train_data[train_data.columns] = scaler.transform(train_data)\n",
    "evaluation_data[evaluation_data.columns] = scaler.transform(evaluation_data)\n",
    "\n",
    "validation_data, test_data = model_selection.train_test_split(evaluation_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "train_dataset = TimeseriesDataset(train_data, sequence_length)\n",
    "test_dataset = TimeseriesDataset(test_data, sequence_length)\n",
    "validation_dataset = TimeseriesDataset(validation_data, sequence_length)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bce6ed",
   "metadata": {
    "id": "2d8fd0ac"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size = data_dimension, hidden_size = hidden_size, num_layers=1, dropout=0.2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, data_dimension)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, input_sequences):\n",
    "        input_sequences = self.drop(input_sequences)\n",
    "        lstm_output, hidden_cell = self.lstm(input_sequences)\n",
    "        res = self.linear(hidden_cell[0][-1])\n",
    "        res = res.view(res.shape[0], 1, -1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17254e4",
   "metadata": {
    "id": "46ef783f"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size = data_dimension, hidden_size = hidden_size, num_layers=1, dropout=0.2, batch_first=True)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, input_sequences):\n",
    "        input_sequences = self.drop(input_sequences)\n",
    "        lstm_output, hidden_cell = self.lstm(input_sequences)\n",
    "        res = self.linear(hidden_cell[0][-1])\n",
    "        res = res.view(res.shape[0], 1, -1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388a8b3",
   "metadata": {
    "id": "b3c12fa0"
   },
   "outputs": [],
   "source": [
    "def model_rmse(model, dataloader, epoch, plot_graph=True, plot_title='TSGAN Prediction', show_preds=True):\n",
    "    rmse = 0\n",
    "    squared_error_list = []\n",
    "    actual_data_list = []\n",
    "    predicted_data_list = []\n",
    "    file_title = plot_title.lower().replace(\" \", \"_\")\n",
    "    \n",
    "    for i, sequence_batch in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            real_sequence = sequence_batch\n",
    "            generator_input_sequence = sequence_batch[:, :-1].to(device)\n",
    "            real_values = sequence_batch[:, -1:]\n",
    "            predicted_values = generator(generator_input_sequence).cpu()\n",
    "            actual_data_list.append(real_values)\n",
    "            predicted_data_list.append(predicted_values)\n",
    "    \n",
    "    real_data = torch.cat(actual_data_list, 0)\n",
    "    predicted_data = torch.cat(predicted_data_list, 0)\n",
    "    \n",
    "    df_pred = pd.DataFrame(predicted_data.view(-1, len(training_columns_list)), columns = training_columns_list)\n",
    "    df_pred_unscaled = pd.DataFrame(scaler.inverse_transform(df_pred), columns = training_columns_list)\n",
    "    df_real = pd.DataFrame(real_data.view(-1, len(training_columns_list)), columns = training_columns_list)\n",
    "    df_real_unscaled = pd.DataFrame(scaler.inverse_transform(df_real), columns = training_columns_list)\n",
    "    \n",
    "    if plot_graph:\n",
    "        if not os.path.exists('./plots_gan/'):\n",
    "            os.makedirs('./plots_gan/')\n",
    "        \n",
    "        for column in training_columns_list:\n",
    "            fig, ax = plt.subplots(figsize=(10,5), dpi=100)\n",
    "            plt.xlabel('Date'); plt.ylabel(column)\n",
    "            plt.title('Time Series Generative Adversarial Network' + f\" -{column}-\")\n",
    "            plt.plot(df_real_unscaled[column], label = 'Actual')\n",
    "            plt.plot(df_pred_unscaled[column], label = 'Predicted')\n",
    "            plt.legend(); ax.yaxis.set_label_position(\"right\"); \n",
    "            \n",
    "            if show_preds and column == 'Close':\n",
    "                plt.show()\n",
    "            # fig.savefig(f'./plots_gan/{file_title}_plt_{column}_e{epoch}.png')\n",
    "            plt.close(fig)\n",
    "            \n",
    "    rmse_results = {}\n",
    "    for column in training_columns_list:\n",
    "        rmse = np.sqrt(((df_real_unscaled[column] - df_pred_unscaled[column])**2).mean())\n",
    "        rmse_results[column] = rmse\n",
    "    return rmse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353fb7f3",
   "metadata": {
    "id": "d4c51fa7"
   },
   "outputs": [],
   "source": [
    "generator = Generator(hidden_size = data_dimension * 2).to(device)\n",
    "discriminator = Discriminator(hidden_size = data_dimension * 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b355310f",
   "metadata": {
    "id": "95164560"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer_generator = optim.Adam(generator.parameters(), lr=learning_rate, betas=optimizer_betas)\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=optimizer_betas)\n",
    "\n",
    "real_label = 1.\n",
    "fake_label = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df0196",
   "metadata": {
    "id": "0d56cf9b"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./models_gan/'):\n",
    "    os.makedirs('./models_gan/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec327d6",
   "metadata": {
    "id": "d8b20864"
   },
   "outputs": [],
   "source": [
    "best_predictor = None\n",
    "min_close_rmse = math.inf\n",
    "\n",
    "evaluation_metrics = {'Generator_loss':[], 'Discriminator_loss':[], 'rmse_values':{}}\n",
    "for column in training_columns_list:\n",
    "        evaluation_metrics['rmse_values'][column] = []\n",
    "                      \n",
    "print('Training started !')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, sequence_batch in enumerate(train_dataloader):\n",
    "            discriminator.zero_grad()\n",
    "            real_sequence = sequence_batch.to(device)\n",
    "            batch_size = real_sequence.size(0)\n",
    "            real_labels = torch.full((batch_size,), real_label, dtype = torch.float, device = device)\n",
    "            discriminator_output_real = discriminator(real_sequence).view(-1)\n",
    "            discriminator_error_real = criterion(discriminator_output_real, real_labels)\n",
    "            discriminator_error_real.backward()\n",
    "\n",
    "            generator_input_sequence = sequence_batch[:,:-1].to(device)\n",
    "            generated_values = generator(generator_input_sequence)\n",
    "            fake_labels = torch.full((batch_size,), fake_label, dtype = torch.float, device = device)\n",
    "            generator_result_concat = torch.cat((generator_input_sequence, generated_values.detach()), 1)\n",
    "            discriminator_output_fake = discriminator(generator_result_concat).view(-1)\n",
    "            discriminator_error_fake = criterion(discriminator_output_fake, fake_labels)\n",
    "            discriminator_error_fake.backward()\n",
    "            discriminator_error = discriminator_error_real + discriminator_error_fake\n",
    "            optimizer_discriminator.step()\n",
    "\n",
    "            generator.zero_grad()\n",
    "            real_labels = torch.full((batch_size,), real_label, dtype = torch.float, device=device)\n",
    "            generator_result_concat_grad = torch.cat((generator_input_sequence, generated_values), 1)\n",
    "            discriminator_output_fake = discriminator(generator_result_concat_grad).view(-1)\n",
    "            generator_error = criterion(discriminator_output_fake, real_labels)\n",
    "            generator_error.backward()\n",
    "            optimizer_generator.step()\n",
    "            \n",
    "    if (epoch+1) % evaluation_epoch_num == 0 or epoch+1 == 1:\n",
    "        rmse_values = model_rmse(generator, validation_dataloader, epoch = (epoch+1), plot_graph=False, show_preds=False)\n",
    "        if rmse_values['Close'] < min_close_rmse:\n",
    "            min_close_rmse = rmse_values['Close']\n",
    "            best_predictor = epoch+1\n",
    "            \n",
    "        for column in training_columns_list:\n",
    "            evaluation_metrics['rmse_values'][column].append(rmse_values[column])\n",
    "            \n",
    "        evaluation_metrics['Generator_loss'].append(generator_error.item())\n",
    "        evaluation_metrics['Discriminator_loss'].append(discriminator_error.item())\n",
    "        \n",
    "        print('\\n[{}/{}]\\tDiscriminator Loss: {:.4f}\\tGenerator Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, discriminator_error.item(), generator_error.item()))\n",
    "        \n",
    "        for col_name, rmse in rmse_values.items():\n",
    "            print(f\"{col_name} RMSE: {rmse:.4f}\")\n",
    "        save_path = os.path.join(\"./models_gan/\",\"model_epoch_{}.pt\".format(epoch+1))\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'generator_model_state_dict': generator.state_dict(),\n",
    "            'discriminator_model_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_generator_state_dict': optimizer_generator.state_dict(),\n",
    "            'optimizer_discriminator_state_dict': optimizer_discriminator.state_dict(),\n",
    "            'discriminator_loss': discriminator_error,\n",
    "            'generator_loss': generator_error,\n",
    "            }, save_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
